{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Text Data For Machine Learning\n",
    "by **Dane Arnesen** on November 4, 2017. \n",
    "\n",
    "Our dataset is a collection of medical texts that we will use to classify different variants of cancer mutations. Because we cannot feed raw text into any kind of machine learning algorithm, we must first do some preprocessing to convert the text into some numeric representation. Below is a list of the high-level preprocessing tasks:\n",
    "- Split the data into train and test sets \n",
    "- Separate the raw text on white space into individual words\n",
    "- Remove all punctuation\n",
    "- Set all words to lowercase\n",
    "- Remove all words that are not purely compromised of alphabetical characters\n",
    "- Remove stop words\n",
    "- Perform stemming\n",
    "- Remove words that are not at least 2 characters in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "First we need to load the raw data from file. We will use the pandas package to assist us with this task. There are actually two files that we need to load: \n",
    "1. **Training Variants**: contains the genetic mutation class (the target) \n",
    "2. **Training Text**: contains the raw medical text \n",
    "\n",
    "Both files contain a field called **ID** which we can use to join the files together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 4)\n",
      "(3321, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    " \n",
    "# Set the working directory for the project\n",
    "os.chdir('C://Users/Dane/Documents/GitHub/seis735_project/')\n",
    "\n",
    "# Training variants\n",
    "variants = pd.read_csv(\"data/raw/training_variants\")\n",
    "\n",
    "# Load the data from file\n",
    "text = pd.read_csv(\"data/raw/training_text\", \n",
    "                   sep=\"\\|\\|\", \n",
    "                   header=None, \n",
    "                   skiprows=1, \n",
    "                   names=[\"ID\",\"Text\"],\n",
    "                   engine=\"python\"\n",
    "                  )\n",
    "\n",
    "print(variants.shape)\n",
    "print(text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Variants and Text\n",
    "Now that the two files have been loaded into memory, we merge the files together using the **ID** field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 5)\n",
      "ID            int64\n",
      "Gene         object\n",
      "Variation    object\n",
      "Class         int64\n",
      "Text         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Use inner join to merge the datasets on ID\n",
    "merged = pd.merge(left=variants, right=text, how=\"inner\", on=\"ID\")\n",
    "\n",
    "# Dropping the variants and text datasets as we won't need them anymore\n",
    "del variants, text\n",
    "\n",
    "print(merged.shape)\n",
    "print(merged.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an idea of what the merged dataframe looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>CBL</td>\n",
       "      <td>V391I</td>\n",
       "      <td>4</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>CBL</td>\n",
       "      <td>V430M</td>\n",
       "      <td>5</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>1</td>\n",
       "      <td>CBL is a negative regulator of activated recep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Y371H</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract Juvenile myelomonocytic leukemia (JM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>CBL</td>\n",
       "      <td>C384R</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract Juvenile myelomonocytic leukemia (JM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class  \\\n",
       "0   0  FAM58A  Truncating Mutations      1   \n",
       "1   1     CBL                 W802*      2   \n",
       "2   2     CBL                 Q249E      2   \n",
       "3   3     CBL                 N454D      3   \n",
       "4   4     CBL                 L399V      4   \n",
       "5   5     CBL                 V391I      4   \n",
       "6   6     CBL                 V430M      5   \n",
       "7   7     CBL              Deletion      1   \n",
       "8   8     CBL                 Y371H      4   \n",
       "9   9     CBL                 C384R      4   \n",
       "\n",
       "                                                Text  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "1   Abstract Background  Non-small cell lung canc...  \n",
       "2   Abstract Background  Non-small cell lung canc...  \n",
       "3  Recent evidence has demonstrated that acquired...  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...  \n",
       "5  Oncogenic mutations in the monomeric Casitas B...  \n",
       "6  Oncogenic mutations in the monomeric Casitas B...  \n",
       "7  CBL is a negative regulator of activated recep...  \n",
       "8   Abstract Juvenile myelomonocytic leukemia (JM...  \n",
       "9   Abstract Juvenile myelomonocytic leukemia (JM...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Train and Test Sets\n",
    "Our model needs to be able to have predictive power on previously unseen data. It is important that we do not train our model using the entire dataset, because this will give our model prior knowledge of the validation data. In other words, we run into a leaky data problem. Therefore, when preparing the data to train the model, we constrain it to only the designated training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5)\n",
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train, test = train_test_split(merged, test_size=0.1, random_state=20171104)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>769</td>\n",
       "      <td>ERBB2</td>\n",
       "      <td>S310Y</td>\n",
       "      <td>7</td>\n",
       "      <td>A 58-year-old woman was evaluated at our insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>1640</td>\n",
       "      <td>FLT3</td>\n",
       "      <td>FLT3 internal tandem duplications</td>\n",
       "      <td>7</td>\n",
       "      <td>Internal tandem duplications of the FMS-like t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>2635</td>\n",
       "      <td>BRCA1</td>\n",
       "      <td>P1856T</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract  The BRCA1 gene from individuals at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2706</td>\n",
       "      <td>BRAF</td>\n",
       "      <td>K483E</td>\n",
       "      <td>2</td>\n",
       "      <td>Precision medicine approaches are ideally suit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>2113</td>\n",
       "      <td>SRC</td>\n",
       "      <td>Amplification</td>\n",
       "      <td>2</td>\n",
       "      <td>The non-receptor tyrosine kinase c-Src, hereaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>2290</td>\n",
       "      <td>STAT3</td>\n",
       "      <td>S614R</td>\n",
       "      <td>7</td>\n",
       "      <td>Lymphomas arising from NK or gd-T cells are ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>2834</td>\n",
       "      <td>BRCA2</td>\n",
       "      <td>K2950N</td>\n",
       "      <td>5</td>\n",
       "      <td>Mutation screening of the breast and ovarian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3233</th>\n",
       "      <td>3233</td>\n",
       "      <td>NTRK2</td>\n",
       "      <td>R715G</td>\n",
       "      <td>5</td>\n",
       "      <td>Purpose  TrkB has been involved in poor cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>736</td>\n",
       "      <td>ERBB2</td>\n",
       "      <td>T733I</td>\n",
       "      <td>2</td>\n",
       "      <td>Purpose: Mutations associated with resistance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2044</td>\n",
       "      <td>IL7R</td>\n",
       "      <td>T244_I245insCPT</td>\n",
       "      <td>7</td>\n",
       "      <td>Signaling mediated by IL-7 and IL-7R is essent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>3176</td>\n",
       "      <td>HNF1A</td>\n",
       "      <td>T196A</td>\n",
       "      <td>3</td>\n",
       "      <td>Heterozygous mutations in the gene encoding he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>2193</td>\n",
       "      <td>PTEN</td>\n",
       "      <td>R15K</td>\n",
       "      <td>4</td>\n",
       "      <td>The PTEN (phosphatase and tensin homolog) phos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>1078</td>\n",
       "      <td>FOXA1</td>\n",
       "      <td>M253K</td>\n",
       "      <td>1</td>\n",
       "      <td>FoxA1 (FOXA1) is a pioneering transcription fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>862</td>\n",
       "      <td>ABL1</td>\n",
       "      <td>F359C</td>\n",
       "      <td>2</td>\n",
       "      <td>For patients with chronic myeloid leukemia who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>1262</td>\n",
       "      <td>PIK3R1</td>\n",
       "      <td>K459_S460delinsN</td>\n",
       "      <td>4</td>\n",
       "      <td>ABSTRACT Cancer-specific mutations in the iSH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    Gene                          Variation  Class  \\\n",
       "769    769   ERBB2                              S310Y      7   \n",
       "1640  1640    FLT3  FLT3 internal tandem duplications      7   \n",
       "2635  2635   BRCA1                             P1856T      6   \n",
       "2706  2706    BRAF                              K483E      2   \n",
       "2113  2113     SRC                      Amplification      2   \n",
       "2290  2290   STAT3                              S614R      7   \n",
       "2834  2834   BRCA2                             K2950N      5   \n",
       "3233  3233   NTRK2                              R715G      5   \n",
       "736    736   ERBB2                              T733I      2   \n",
       "2044  2044    IL7R                    T244_I245insCPT      7   \n",
       "3176  3176   HNF1A                              T196A      3   \n",
       "2193  2193    PTEN                               R15K      4   \n",
       "1078  1078   FOXA1                              M253K      1   \n",
       "862    862    ABL1                              F359C      2   \n",
       "1262  1262  PIK3R1                   K459_S460delinsN      4   \n",
       "\n",
       "                                                   Text  \n",
       "769   A 58-year-old woman was evaluated at our insti...  \n",
       "1640  Internal tandem duplications of the FMS-like t...  \n",
       "2635   Abstract  The BRCA1 gene from individuals at ...  \n",
       "2706  Precision medicine approaches are ideally suit...  \n",
       "2113  The non-receptor tyrosine kinase c-Src, hereaf...  \n",
       "2290  Lymphomas arising from NK or gd-T cells are ve...  \n",
       "2834  Mutation screening of the breast and ovarian c...  \n",
       "3233  Purpose  TrkB has been involved in poor cancer...  \n",
       "736   Purpose: Mutations associated with resistance ...  \n",
       "2044  Signaling mediated by IL-7 and IL-7R is essent...  \n",
       "3176  Heterozygous mutations in the gene encoding he...  \n",
       "2193  The PTEN (phosphatase and tensin homolog) phos...  \n",
       "1078  FoxA1 (FOXA1) is a pioneering transcription fa...  \n",
       "862   For patients with chronic myeloid leukemia who...  \n",
       "1262   ABSTRACT Cancer-specific mutations in the iSH...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the datasets, we want to verify we have relatively similar distribution of the target class in both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Class\n",
      "Class           \n",
      "1      17.068273\n",
      "2      13.386881\n",
      "3       2.710843\n",
      "4      20.649264\n",
      "5       7.463186\n",
      "6       8.467202\n",
      "7      28.781794\n",
      "8       0.468541\n",
      "9       1.004016\n",
      "\n",
      "           Class\n",
      "Class           \n",
      "1      17.417417\n",
      "2      15.615616\n",
      "3       2.402402\n",
      "4      20.720721\n",
      "5       5.705706\n",
      "6       6.606607\n",
      "7      27.927928\n",
      "8       1.501502\n",
      "9       2.102102\n"
     ]
    }
   ],
   "source": [
    "# Print the number of each class in each dataset. \n",
    "print(train.groupby('Class').agg({'Class':'count'}).apply(lambda x: 100.0 * x / float(x.sum())))\n",
    "print()\n",
    "print(test.groupby('Class').agg({'Class':'count'}).apply(lambda x: 100.0 * x / float(x.sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our Vocabulary\n",
    "We will create a function called **clean_doc** that will perform the following tasks:\n",
    "- Separate the raw text on white space into individual words \n",
    "- Remove all punctuation \n",
    "- Set all words to lowercase\n",
    "- Remove all words that are not purely compromised of alphabetical characters\n",
    "- Remove stop words\n",
    "- Perform stemming\n",
    "- Remove words that are not at least 2 characters in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Function that turns a doc into clean tokens\n",
    "def clean_doc(doc, stemmer, stop_words):\n",
    "    # Split into individual tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation and set to lowercase\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table).lower() for w in tokens]\n",
    "    # Remove words that are not entirely alphabetical\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # Removing all known stop words\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove tokens that aren't at least two characters in length\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    # Stem the remaining tokens\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a distinct list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize a stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We've built a function that will do pre-processing on a single text. We want to apply that function to our entire collection of texts in order to build our vocabulary. We will develop our vocabulary as a counter, which is a dictionary that maps words to their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in train['Text']:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Add tokens to vocab\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 87330\n",
      "\n",
      "[('mutat', 327526), ('cell', 266388), ('activ', 168430), ('mutant', 107869), ('protein', 107245), ('express', 106428), ('use', 103634), ('tumor', 102478), ('cancer', 100389), ('et', 96765), ('patient', 96397), ('figur', 92763), ('al', 92599), ('fig', 92463), ('gene', 88950), ('variant', 81290), ('domain', 74107), ('function', 69263), ('result', 67523), ('studi', 65508)]\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the vocab\n",
    "print('Size of the vocabulary: %d' % len(vocab))\n",
    "print()\n",
    "\n",
    "# Print the 20 most common words in the vocab\n",
    "print(vocab.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove all words from the vocabulary that occur very infrequently. These words are likely not very important to making any sort of prediction. For the time being we'll remove words that do not occur at least n times. This reduces our vocabulary by almost half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23899\n"
     ]
    }
   ],
   "source": [
    "# Removing words that do not occur at least n number of times in the vocabulary\n",
    "tokens = [k for k,c in vocab.items() if c >= 10]\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our vocabulary to a text file so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# Save tokens to a vocabulary file\n",
    "save_list(tokens, 'data/interim/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Text As A Vector\n",
    "In order to feed our text data into a machine learning algorithm like XGBoost or neural networks, it needs to be converted from text into a numerical vector. Each medical text will be converted into a vector, where the number of items in the vector corresponds to the words in our previously defined vocabulary. Most importantly, each word in the text will be scored. There are multiple scoring methods, including but not limited to: \n",
    "- Binary: Words are marked as present (1) or not present (0)\n",
    "- Count: Counts the occurrence of each word in the document\n",
    "- Tfidf: Scores each word based on occurrence within a document and across all documents. Words that are frequent across many documents will receive a lower score.\n",
    "- Frequency: Scores each word based on their frequency of occurrence within the document\n",
    "\n",
    "First, load the vocab that we previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23899\n"
     ]
    }
   ],
   "source": [
    "# Open the vocab file\n",
    "file = open('data/interim/vocab.txt', 'r')\n",
    "\n",
    "# Read the vocab from file\n",
    "vocab = file.read()\n",
    "\n",
    "# Close the file\n",
    "file.close()\n",
    "\n",
    "# Unique list of our vocab\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to filter each of our texts down to only the words in our defined vocabulary. Important note, although we've filtered out all of the words that aren't in our defined vocabulary, all of the words are still in order of occurrence in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2988\n"
     ]
    }
   ],
   "source": [
    "# A container object that will hold the words of each individual document\n",
    "lines = list()\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in train['Text']:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Filter the words in the document by our defined vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # Concatentate each word in the document by a single space and append to our lines container\n",
    "    lines.append(' '.join(tokens))\n",
    "\n",
    "# Printing the size of our lines object. It should be 2,988 in length\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman evalu institut mana\n",
      "intern tandem duplic fmsl\n",
      "abstract gene individu ri\n",
      "precis medicin approach i\n",
      "nonreceptor tyrosin kinas\n",
      "lymphoma aris nk gdt cell\n",
      "mutat screen breast ovari\n",
      "purpos trkb involv poor c\n",
      "purpos mutat associ resis\n",
      "signal mediat essenti nor\n",
      "heterozyg mutat gene enco\n",
      "pten phosphatas tensin ho\n",
      "pioneer transcript factor\n",
      "patient chronic myeloid l\n",
      "abstract cancerspecif mut\n",
      "protein kinas braf mutat \n",
      "abstract gene individu ri\n",
      "tyrosin kinas domain muta\n",
      "mutat account major hered\n",
      "publish analys effect mis\n",
      "hallmark stem cell myelop\n",
      "screen tumor suppressor g\n",
      "kinaseakt pathway promot \n",
      "gefitinib effect firstlin\n",
      "purpos kit major oncogen \n",
      "forkhead box fox superfam\n",
      "era person medicin unders\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(lines):\n",
    "    print(l[:25])\n",
    "    if i > 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleansed training text in case we want to reference it later. The cleansing step takes a long time, so it would help if we could avoid repeating that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Pickle the cleansed training text\n",
    "with open('models/training_text.pickle', 'wb') as output:\n",
    "    pickle.dump(lines, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the test data using the exact same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n"
     ]
    }
   ],
   "source": [
    "# A container object that will hold the words of each individual document\n",
    "lines_test = list()\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in test['Text']:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Filter the words in the document by our defined vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # Concatentate each word in the document by a single space and append to our lines container\n",
    "    lines_test.append(' '.join(tokens))\n",
    "\n",
    "# Printing the size of our lines object. It should be 2,988 in length\n",
    "print(len(lines_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nuclear factor erythroid \n",
      "cancer aris owe mutat sub\n",
      "tumor suppressor protein \n",
      "inherit mutat affect locu\n",
      "compar genom hybrid cgh r\n",
      "util genet screen yeast s\n",
      "cancer genom character ef\n",
      "activ cyclin ddepend kina\n",
      "background numer biolog e\n",
      "oxid electrophil stress s\n",
      "famili adenomat polyposi \n",
      "alter ofth egfr gene occu\n",
      "sequencespecif dna bind e\n",
      "graphic abstract imag unl\n",
      "report fusion monocyt leu\n",
      "pten phosphatas tensin ho\n",
      "dna helicas directli inte\n",
      "rhabdomyosarcoma rm child\n",
      "cell lung cancer nsclc di\n",
      "function character cancer\n",
      "fusion defin subset lung \n",
      "purpos plateletderiv grow\n",
      "abstract classif rare mis\n",
      "mutat caus inactiv tumor \n",
      "cancer develop acquisit c\n",
      "function character cancer\n",
      "gene encod transcript act\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(lines_test):\n",
    "    print(l[:25])\n",
    "    if i > 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the cleansed test text\n",
    "with open('models/test_text.pickle', 'wb') as output:\n",
    "    pickle.dump(lines_test, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras to help us wrap up our preprocessing tasks. Keras has a class called **Tokenizer** which can help us encode our data in matrix format using the four different methods we described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Instantiate a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving any further, we should save the trained tokenizer object so we can reference it again at a later point in time. We'll use pickle to save the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Pickle the tokenizer\n",
    "with open('models/tokenizer.pickle', 'wb') as output:\n",
    "    pickle.dump(tokenizer, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Load our tokenizer object that was already trained \n",
    "#with open('models/tokenizer.pickle', 'rb') as obj:\n",
    "#    tokenizer = pickle.load(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's encode our data using the **Frequency** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 23900)\n"
     ]
    }
   ],
   "source": [
    "train_vector = tokenizer.texts_to_matrix(lines, mode='freq')\n",
    "print(train_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03722804,  0.02127317,  0.0069299 ,  0.00918614],\n",
       "       [ 0.02322581,  0.0116129 ,  0.01225806,  0.00193548],\n",
       "       [ 0.04157667,  0.00215983,  0.01781857,  0.00809935],\n",
       "       [ 0.01090151,  0.0132714 ,  0.03507441,  0.01516731],\n",
       "       [ 0.00209986,  0.02438171,  0.02449837,  0.00116659],\n",
       "       [ 0.02727542,  0.0444708 ,  0.00770827,  0.01156241],\n",
       "       [ 0.02140613,  0.00039277,  0.00078555,  0.        ],\n",
       "       [ 0.0265596 ,  0.05188388,  0.01050031,  0.01050031],\n",
       "       [ 0.05757242,  0.02181885,  0.01210121,  0.00311698],\n",
       "       [ 0.02622951,  0.03636364,  0.004769  ,  0.01639344],\n",
       "       [ 0.0367878 ,  0.01435621,  0.01076716,  0.01570211],\n",
       "       [ 0.04830508,  0.00988701,  0.02542373,  0.0019774 ],\n",
       "       [ 0.00705171,  0.05372733,  0.00235057,  0.00436535],\n",
       "       [ 0.01406534,  0.00181488,  0.00317604,  0.00045372],\n",
       "       [ 0.02444324,  0.01792504,  0.01412276,  0.03910918],\n",
       "       [ 0.01983038,  0.01234722,  0.05861811,  0.0193315 ],\n",
       "       [ 0.02125057,  0.00305764,  0.01696988,  0.00397493],\n",
       "       [ 0.0311174 ,  0.00707214,  0.02074493,  0.00801509],\n",
       "       [ 0.01384482,  0.01644073,  0.00634554,  0.00490337],\n",
       "       [ 0.01140653,  0.02071626,  0.00889038,  0.00452906]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector[0:20, 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the matrix to a dataframe and export to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 23899)\n",
      "(2988, 5)\n",
      "(2988, 23904)\n",
      "(2988, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train vector to a dataframe \n",
    "train_df = pd.DataFrame(train_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train.shape)\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "train_final = pd.concat([train.reset_index(drop=True), train_df], axis=1)\n",
    "\n",
    "print(train_final.shape)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "train_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>woman</th>\n",
       "      <th>evalu</th>\n",
       "      <th>institut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>0.006930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.023226</td>\n",
       "      <td>0.011613</td>\n",
       "      <td>0.012258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.041577</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.017819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>0.035074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.024498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.044471</td>\n",
       "      <td>0.007708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.051884</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.057572</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>0.012101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.026230</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.004769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.010767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>0.048305</td>\n",
       "      <td>0.009887</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.002351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0.024443</td>\n",
       "      <td>0.017925</td>\n",
       "      <td>0.014123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.012347</td>\n",
       "      <td>0.058618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0.021251</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.016970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.020745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0.013845</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>0.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.008890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class     woman     evalu  institut\n",
       "0       7  0.037228  0.021273  0.006930\n",
       "1       7  0.023226  0.011613  0.012258\n",
       "2       6  0.041577  0.002160  0.017819\n",
       "3       2  0.010902  0.013271  0.035074\n",
       "4       2  0.002100  0.024382  0.024498\n",
       "5       7  0.027275  0.044471  0.007708\n",
       "6       5  0.021406  0.000393  0.000786\n",
       "7       5  0.026560  0.051884  0.010500\n",
       "8       2  0.057572  0.021819  0.012101\n",
       "9       7  0.026230  0.036364  0.004769\n",
       "10      3  0.036788  0.014356  0.010767\n",
       "11      4  0.048305  0.009887  0.025424\n",
       "12      1  0.007052  0.053727  0.002351\n",
       "13      2  0.014065  0.001815  0.003176\n",
       "14      4  0.024443  0.017925  0.014123\n",
       "15      7  0.019830  0.012347  0.058618\n",
       "16      4  0.021251  0.003058  0.016970\n",
       "17      2  0.031117  0.007072  0.020745\n",
       "18      5  0.013845  0.016441  0.006346\n",
       "19      4  0.011407  0.020716  0.008890"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final[['Class','woman','evalu','institut']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing some cleanup\n",
    "del train_df, train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Export the dataframe to a compressed file\n",
    "train_final.to_csv('data/interim/train_freq.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the test data to matrix format using the frequency method\n",
    "test_vector = tokenizer.texts_to_matrix(lines_test, mode='freq')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "test_df = pd.DataFrame(test_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "test_final = pd.concat([test.reset_index(drop=True), test_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "test_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(test_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del test_df, test_vector\n",
    "\n",
    "# Export the dataframe to a csv file\n",
    "test_final.to_csv('data/interim/test_freq.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode our data using the **binary** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train data to matrix format using the binary method\n",
    "train_vector = tokenizer.texts_to_matrix(lines, mode='binary')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "train_df = pd.DataFrame(train_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "train_final = pd.concat([train.reset_index(drop=True), train_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "train_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(train_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del train_df, train_vector\n",
    "\n",
    "# Export the dataframe to a compressed file\n",
    "train_final.to_csv('data/interim/train_binary.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the test data to matrix format using the binary method\n",
    "test_vector = tokenizer.texts_to_matrix(lines_test, mode='binary')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "test_df = pd.DataFrame(test_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "test_final = pd.concat([test.reset_index(drop=True), test_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "test_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(test_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del test_df, test_vector\n",
    "\n",
    "# Export the dataframe to a compressed file\n",
    "test_final.to_csv('data/interim/test_binary.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode our data using the **Tfidf** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train data to matrix format using the tfidf method\n",
    "train_vector = tokenizer.texts_to_matrix(lines, mode='tfidf')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "train_df = pd.DataFrame(train_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "train_final = pd.concat([train.reset_index(drop=True), train_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "train_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(train_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del train_df, train_vector\n",
    "\n",
    "# Export the dataframe to a compressed file\n",
    "train_final.to_csv('data/interim/train_tfidf.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the test data to matrix format using the tfidf method\n",
    "test_vector = tokenizer.texts_to_matrix(lines_test, mode='tfidf')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "test_df = pd.DataFrame(test_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "test_final = pd.concat([test.reset_index(drop=True), test_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "test_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(test_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del test_df, test_vector\n",
    "\n",
    "# Export the dataframe to a compressed file\n",
    "test_final.to_csv('data/interim/test_tfidf.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode our data using the **Count** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train data to matrix format using the count method\n",
    "train_vector = tokenizer.texts_to_matrix(lines, mode='count')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "train_df = pd.DataFrame(train_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "train_final = pd.concat([train.reset_index(drop=True), train_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "train_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(train_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del train_df, train_vector\n",
    "\n",
    "# Export the dataframe to a compressed file\n",
    "train_final.to_csv('data/interim/train_count.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 23903)\n"
     ]
    }
   ],
   "source": [
    "# Convert the test data to matrix format using the count method\n",
    "test_vector = tokenizer.texts_to_matrix(lines_test, mode='count')\n",
    "\n",
    "# Convert the train vector to a dataframe \n",
    "test_df = pd.DataFrame(test_vector[:,1:], columns=[key for key in tokenizer.word_counts])\n",
    "\n",
    "# Merge the original train dataset to the vectorized dataset\n",
    "test_final = pd.concat([test.reset_index(drop=True), test_df], axis=1)\n",
    "\n",
    "# Drop the text blob from the original train dataset\n",
    "test_final.drop(['Text'], inplace=True, axis=1)\n",
    "\n",
    "# Confirm the dataframe shape\n",
    "print(test_final.shape)\n",
    "\n",
    "# Doing some cleanup\n",
    "del test_df, test_vector\n",
    "\n",
    "# Export the dataframe to a compressed file\n",
    "test_final.to_csv('data/interim/test_count.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
