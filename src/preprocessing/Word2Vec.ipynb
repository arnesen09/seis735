{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding Using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 4)\n",
      "(3321, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    " \n",
    "# Set the working directory for the project\n",
    "os.chdir('C://Users/Dane/Documents/GitHub/seis735_project/')\n",
    "\n",
    "# Training variants\n",
    "variants = pd.read_csv(\"data/raw/training_variants\")\n",
    "\n",
    "# Load the data from file\n",
    "text = pd.read_csv(\"data/raw/training_text\", \n",
    "                   sep=\"\\|\\|\", \n",
    "                   header=None, \n",
    "                   skiprows=1, \n",
    "                   names=[\"ID\",\"Text\"],\n",
    "                   engine=\"python\"\n",
    "                  )\n",
    "\n",
    "print(variants.shape)\n",
    "print(text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Variants and Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 5)\n",
      "ID            int64\n",
      "Gene         object\n",
      "Variation    object\n",
      "Class         int64\n",
      "Text         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Use inner join to merge the datasets on ID\n",
    "merged = pd.merge(left=variants, right=text, how=\"inner\", on=\"ID\")\n",
    "\n",
    "# Dropping the variants and text datasets as we won't need them anymore\n",
    "del variants, text\n",
    "\n",
    "print(merged.shape)\n",
    "print(merged.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class  \\\n",
       "0   0  FAM58A  Truncating Mutations      1   \n",
       "1   1     CBL                 W802*      2   \n",
       "2   2     CBL                 Q249E      2   \n",
       "3   3     CBL                 N454D      3   \n",
       "4   4     CBL                 L399V      4   \n",
       "\n",
       "                                                Text  \n",
       "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
       "1   Abstract Background  Non-small cell lung canc...  \n",
       "2   Abstract Background  Non-small cell lung canc...  \n",
       "3  Recent evidence has demonstrated that acquired...  \n",
       "4  Oncogenic mutations in the monomeric Casitas B...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5)\n",
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train, test = train_test_split(merged, test_size=0.1, random_state=20171104)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences into Tokens\n",
    "Note I've already defined our vocab and cleansed our texts. Let's load the already cleansed texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2988\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Import the pre-defined training tokens\n",
    "with open('models/training_text.pickle', 'rb') as obj:\n",
    "    texts_train = pickle.load(obj)\n",
    "    \n",
    "# Import the pre-defined test tokens\n",
    "with open('models/test_text.pickle', 'rb') as obj:\n",
    "    texts_test = pickle.load(obj)\n",
    "\n",
    "# Printing the size of our lines object. It should be 2,988 in length\n",
    "print(len(texts_train))\n",
    "print(len(texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the text into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2988\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "tokens_train = [line.split() for line in texts_train]\n",
    "tokens_test = [line.split() for line in texts_test]\n",
    "\n",
    "print(len(tokens_train))\n",
    "print(len(tokens_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Gensim to Perform Text Embedding\n",
    "First we train the word2vec model on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=23899, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train our model on the train_tokens\n",
    "model = Word2Vec(tokens_train, min_count=10, size=300)\n",
    "\n",
    "# Summarize the model\n",
    "print(model)\n",
    "\n",
    "# Save the model\n",
    "model.save('models/word2vec_train.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the vector weights from the trained embedding model. These weights will be used to create a Keras Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = model.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding matrix representation of the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23900, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load our trained word2vec model\n",
    "model = Word2Vec.load('models/word2vec_train.bin')\n",
    "\n",
    "# Size of our vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# convert the wv word vectors into a numpy matrix that is suitable for insertion into our TensorFlow and Keras models\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our previously defined tokenizer, which was trained during our bag-of-words processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import our tokenizer object\n",
    "with open('models/tokenizer.pickle', 'rb') as obj:\n",
    "    tokenizer = pickle.load(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our text data into sequences, and pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_encoded = tokenizer.texts_to_sequences(texts_train)\n",
    "test_encoded = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 20000)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad documents to a max length\n",
    "#max_length = max(len(x) for x in texts_train)\n",
    "#print(max_length)\n",
    "max_length = 20000\n",
    "train_padded = pad_sequences(train_encoded, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_encoded, maxlen=max_length, padding='post')\n",
    "print(train_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before we start training our model is to convert our target attributes into dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988,)\n",
      "(2988, 9)\n",
      "(333,)\n",
      "(333, 9)\n"
     ]
    }
   ],
   "source": [
    "# Convert predictors to matrix format\n",
    "y_train_true = train.as_matrix()[:,0]\n",
    "y_train = pd.get_dummies(train[['Class']], prefix='y', columns=['Class']).as_matrix()\n",
    "\n",
    "y_test_true = test.as_matrix()[:,0]\n",
    "y_test = pd.get_dummies(test[['Class']], prefix='y', columns=['Class']).as_matrix()\n",
    "\n",
    "print(y_train_true.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test_true.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Sequential FF Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 6, 300)            7170000   \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              1801000   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 9)                 9009      \n",
      "=================================================================\n",
      "Total params: 8,980,009\n",
      "Trainable params: 1,810,009\n",
      "Non-trainable params: 7,170,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/50\n",
      " - 12s - loss: 2.2934 - acc: 0.4759 - val_loss: 1.7570 - val_acc: 0.5556\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.0168 - acc: 0.7326 - val_loss: 1.4753 - val_acc: 0.5826\n",
      "Epoch 3/50\n",
      " - 9s - loss: 0.7700 - acc: 0.7590 - val_loss: 1.5284 - val_acc: 0.5315\n",
      "Epoch 4/50\n",
      " - 10s - loss: 0.7048 - acc: 0.7701 - val_loss: 1.4242 - val_acc: 0.5646\n",
      "Epoch 5/50\n",
      " - 8s - loss: 0.6773 - acc: 0.7641 - val_loss: 1.4444 - val_acc: 0.5526\n",
      "Epoch 6/50\n",
      " - 8s - loss: 0.6071 - acc: 0.7744 - val_loss: 1.4518 - val_acc: 0.5435\n",
      "Epoch 7/50\n",
      " - 9s - loss: 0.5892 - acc: 0.7694 - val_loss: 1.4205 - val_acc: 0.5826\n",
      "Epoch 8/50\n",
      " - 8s - loss: 0.5570 - acc: 0.7738 - val_loss: 1.4540 - val_acc: 0.5616\n",
      "Epoch 9/50\n",
      " - 8s - loss: 0.5550 - acc: 0.7754 - val_loss: 1.4175 - val_acc: 0.5345\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.5276 - acc: 0.7805 - val_loss: 1.4601 - val_acc: 0.5285\n",
      "Epoch 11/50\n",
      " - 10s - loss: 0.5320 - acc: 0.7741 - val_loss: 1.4789 - val_acc: 0.5405\n",
      "Epoch 12/50\n",
      " - 11s - loss: 0.5159 - acc: 0.7805 - val_loss: 1.4708 - val_acc: 0.5405\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.5119 - acc: 0.7828 - val_loss: 1.4772 - val_acc: 0.5495\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.5128 - acc: 0.7741 - val_loss: 1.4527 - val_acc: 0.5495\n",
      "Epoch 15/50\n",
      " - 10s - loss: 0.5059 - acc: 0.7821 - val_loss: 1.4896 - val_acc: 0.5646\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.5053 - acc: 0.7764 - val_loss: 1.4897 - val_acc: 0.5646\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.4957 - acc: 0.7788 - val_loss: 1.5023 - val_acc: 0.5526\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.5002 - acc: 0.7845 - val_loss: 1.5306 - val_acc: 0.5526\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.4968 - acc: 0.7818 - val_loss: 1.5051 - val_acc: 0.5586\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2135ae03470>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create the model architecture\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=True)\n",
    "\n",
    "# Checkpoint - used to get the best weights during the model training process\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_weights.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_padded, \n",
    "          y_train, \n",
    "          validation_data=(test_padded, y_test), \n",
    "          epochs=50, \n",
    "          batch_size=32, \n",
    "          callbacks=[early_stopping, checkpoint], \n",
    "          verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of this model is poor. Can we get better performance from a convolutional neural network architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "# Initialize our embedding layer\n",
    "embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "\n",
    "inputs = Input(shape=(max_length,))\n",
    "embedding_seq = embedding(inputs)\n",
    "x = Conv1D(128, 5, activation='relu')(embedding_seq)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(9, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=True)\n",
    "\n",
    "# Checkpoint - used to get the best weights during the model training process\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_weights.h5', monitor='val_acc', save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(train_padded, \n",
    "#          y_train, \n",
    "#          validation_data=(test_padded, y_test), \n",
    "#          epochs=50, \n",
    "#          batch_size=128, \n",
    "#          callbacks=[early_stopping, checkpoint], \n",
    "#          verbose=2\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
