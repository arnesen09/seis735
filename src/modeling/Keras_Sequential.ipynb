{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Bag-of-Words Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point the data has already been preprocessed. The purpose of this notebook is to find the best neural network model using grid-search parameter tuning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "First we need to load the data which has been stored in a compressed format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 23903)\n",
      "(333, 23903)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir('/home/ec2-user/Notebooks/seis735_aws/')\n",
    "\n",
    "# Read the training data\n",
    "df_train = pd.read_csv('data/train_freq.gz', compression='gzip', encoding='ISO-8859-1')\n",
    "df_test = pd.read_csv('data/test_freq.gz', compression='gzip', encoding='ISO-8859-1')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are nine different classes in our target attribute. These classes need to be converted into dummy variables in order to be used in a neural network prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 9)\n",
      "(333, 9)\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.get_dummies(df_train[['Class']], prefix='y', columns=['Class'])\n",
    "y_test = pd.get_dummies(df_test[['Class']], prefix='y', columns=['Class'])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to format the data so it may be ingested by a neural network. This means dropping a few of the meaningless columns, as well as converting the dataframe to a matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the predictors\n",
    "df_train.drop(['ID','Gene','Variation'], inplace=True, axis=1)\n",
    "df_test.drop(['ID','Gene','Variation'], inplace=True, axis=1)\n",
    "\n",
    "# Convert predictors to matrix format\n",
    "x_train = df_train.as_matrix()[:,1:]\n",
    "y_train_true = df_train.as_matrix()[:,0]\n",
    "y_train = y_train.as_matrix()\n",
    "\n",
    "\n",
    "x_test = df_test.as_matrix()[:,1:]\n",
    "y_test_true = df_test.as_matrix()[:,0]\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be fed into a Keras neural network model. We want to train the network in a step-wise fashion using a grid-search technique. Let's define a class to help us with the grid-search. There is a pre-defined sklearn GridSearchCV class which I could have used for this task. However, the customer class below will be much faster as it allows for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import load_model\n",
    "\n",
    "class KerasClassifierGridSearchCV(object):\n",
    "    \"\"\"\n",
    "    Use cross-fold validation to perform parameter tuning on a Keras feed forward neural network.  \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, params, nfolds, seed):\n",
    "        self.params = params\n",
    "        self.nfolds = nfolds\n",
    "        self.seed = seed\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.avg_results = {}\n",
    "    # Return the trained models    \n",
    "    def get_models(self):\n",
    "        return (self.models)\n",
    "    # Return the results for each of the trained models\n",
    "    def get_results(self):\n",
    "        return (self.results)\n",
    "    # Return the overall avg results for each parameter\n",
    "    def get_avg_results(self):\n",
    "        return (self.avg_results)\n",
    "    # Fit 1 model per n-fold per parameter    \n",
    "    def fit(self, build_fn, X_train, Y_train, epochs=50, batch_size=50, early_stopping_rounds=5, min_delta=0.001):\n",
    "        # Split the dataset into k-folds\n",
    "        kf = KFold(n_splits=self.nfolds, random_state=self.seed, shuffle=True)\n",
    "        # Iterate over the parameter grid\n",
    "        for i, p in enumerate(self.params):\n",
    "            print('Training with: %s' % p)\n",
    "            # Initializing empty states\n",
    "            average = []\n",
    "            weights = []\n",
    "            #fold_number = 1\n",
    "            # Iterate over the k-folds\n",
    "            for train_index, test_index in kf.split(X_train):             \n",
    "                # Get the training and test data for each fold\n",
    "                x_train, x_test = X_train[train_index], X_train[test_index]\n",
    "                y_train, y_test = Y_train[train_index], Y_train[test_index]\n",
    "                # Create the estimator\n",
    "                estimator = build_fn(p)\n",
    "                # To make training faster on subsequent folds, use pre-trained weights from the first fold\n",
    "                #if fold_number > 1:\n",
    "                #    estimator.set_weights(w1)\n",
    "                # Early stopping\n",
    "                early_stopping = EarlyStopping(monitor='val_acc', min_delta=min_delta, patience=early_stopping_rounds, verbose=True)\n",
    "                # Train the estimator\n",
    "                estimator.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "                # Get the mse\n",
    "                avg = estimator.evaluate(x_test, y_test)\n",
    "                # Append the mean squared error\n",
    "                average.append(avg[1])\n",
    "                # Save the model\n",
    "                weights.append(estimator)\n",
    "                # Get the trained weights from the first fold\n",
    "                #if fold_number == 1:\n",
    "                #    w1 = estimator.get_weights()\n",
    "                #fold_number += 1\n",
    "            # Save the results and model weights    \n",
    "            self.models[p] = weights\n",
    "            self.results[p] = average\n",
    "            self.avg_results[p] = np.mean(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize the parameters and model that we want to train. The first thing we are going to test is the number of neurons to use in the first hidden layer. We will utilize early stopping, which will cause training to cease if there is no improvement in the validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: 500\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 2.0226 - acc: 0.3007 - val_loss: 1.8745 - val_acc: 0.2530\n",
      "Epoch 2/50\n",
      " - 3s - loss: 1.7669 - acc: 0.3128 - val_loss: 1.7752 - val_acc: 0.3544\n",
      "Epoch 3/50\n",
      " - 3s - loss: 1.6696 - acc: 0.3780 - val_loss: 1.6823 - val_acc: 0.4006\n",
      "Epoch 4/50\n",
      " - 3s - loss: 1.5639 - acc: 0.4488 - val_loss: 1.5912 - val_acc: 0.4157\n",
      "Epoch 5/50\n",
      " - 3s - loss: 1.4592 - acc: 0.4849 - val_loss: 1.4991 - val_acc: 0.4649\n",
      "Epoch 6/50\n",
      " - 3s - loss: 1.3622 - acc: 0.5266 - val_loss: 1.4216 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      " - 3s - loss: 1.2791 - acc: 0.5718 - val_loss: 1.3629 - val_acc: 0.5201\n",
      "Epoch 8/50\n",
      " - 3s - loss: 1.2088 - acc: 0.6024 - val_loss: 1.3192 - val_acc: 0.5321\n",
      "Epoch 9/50\n",
      " - 3s - loss: 1.1458 - acc: 0.6285 - val_loss: 1.2830 - val_acc: 0.5412\n",
      "Epoch 10/50\n",
      " - 3s - loss: 1.0897 - acc: 0.6536 - val_loss: 1.2596 - val_acc: 0.5502\n",
      "Epoch 11/50\n",
      " - 3s - loss: 1.0413 - acc: 0.6667 - val_loss: 1.2494 - val_acc: 0.5452\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.9965 - acc: 0.6767 - val_loss: 1.2062 - val_acc: 0.5572\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.9545 - acc: 0.6933 - val_loss: 1.1906 - val_acc: 0.5723\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.9174 - acc: 0.7103 - val_loss: 1.1719 - val_acc: 0.5833\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.8822 - acc: 0.7254 - val_loss: 1.1591 - val_acc: 0.5944\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.8499 - acc: 0.7324 - val_loss: 1.1444 - val_acc: 0.5733\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.8213 - acc: 0.7420 - val_loss: 1.1373 - val_acc: 0.5894\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.7907 - acc: 0.7485 - val_loss: 1.1243 - val_acc: 0.5894\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.7635 - acc: 0.7575 - val_loss: 1.1220 - val_acc: 0.5964\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.7378 - acc: 0.7636 - val_loss: 1.1183 - val_acc: 0.5954\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.7134 - acc: 0.7656 - val_loss: 1.1198 - val_acc: 0.5863\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.6874 - acc: 0.7721 - val_loss: 1.1004 - val_acc: 0.6004\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.6642 - acc: 0.7786 - val_loss: 1.1043 - val_acc: 0.5924\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.6452 - acc: 0.7866 - val_loss: 1.0931 - val_acc: 0.5964\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.6261 - acc: 0.7942 - val_loss: 1.0915 - val_acc: 0.5924\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.6068 - acc: 0.8022 - val_loss: 1.0982 - val_acc: 0.6024\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.5905 - acc: 0.7997 - val_loss: 1.0967 - val_acc: 0.5884\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.5714 - acc: 0.8072 - val_loss: 1.0871 - val_acc: 0.5954\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.5559 - acc: 0.8102 - val_loss: 1.0825 - val_acc: 0.6014\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.5421 - acc: 0.8193 - val_loss: 1.1007 - val_acc: 0.5914\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.5286 - acc: 0.8233 - val_loss: 1.0972 - val_acc: 0.5994\n",
      "Epoch 00031: early stopping\n",
      "996/996 [==============================] - 0s 306us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 2.0121 - acc: 0.2801 - val_loss: 1.8181 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 3s - loss: 1.7839 - acc: 0.2962 - val_loss: 1.7250 - val_acc: 0.3554\n",
      "Epoch 3/50\n",
      " - 3s - loss: 1.6904 - acc: 0.3599 - val_loss: 1.6402 - val_acc: 0.4367\n",
      "Epoch 4/50\n",
      " - 3s - loss: 1.5829 - acc: 0.4493 - val_loss: 1.5397 - val_acc: 0.4839\n",
      "Epoch 5/50\n",
      " - 3s - loss: 1.4750 - acc: 0.4859 - val_loss: 1.4473 - val_acc: 0.4980\n",
      "Epoch 6/50\n",
      " - 3s - loss: 1.3759 - acc: 0.5442 - val_loss: 1.3742 - val_acc: 0.5371\n",
      "Epoch 7/50\n",
      " - 3s - loss: 1.2908 - acc: 0.5803 - val_loss: 1.3180 - val_acc: 0.5542\n",
      "Epoch 8/50\n",
      " - 3s - loss: 1.2203 - acc: 0.6089 - val_loss: 1.2685 - val_acc: 0.5683\n",
      "Epoch 9/50\n",
      " - 3s - loss: 1.1557 - acc: 0.6320 - val_loss: 1.2303 - val_acc: 0.5723\n",
      "Epoch 10/50\n",
      " - 3s - loss: 1.1003 - acc: 0.6486 - val_loss: 1.2051 - val_acc: 0.5934\n",
      "Epoch 11/50\n",
      " - 3s - loss: 1.0500 - acc: 0.6632 - val_loss: 1.1766 - val_acc: 0.6104\n",
      "Epoch 12/50\n",
      " - 3s - loss: 1.0058 - acc: 0.6742 - val_loss: 1.1550 - val_acc: 0.6064\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.9666 - acc: 0.6857 - val_loss: 1.1362 - val_acc: 0.6114\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.9276 - acc: 0.7033 - val_loss: 1.1214 - val_acc: 0.6145\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.8928 - acc: 0.7008 - val_loss: 1.1133 - val_acc: 0.6145\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.8585 - acc: 0.7139 - val_loss: 1.1073 - val_acc: 0.6155\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.8290 - acc: 0.7224 - val_loss: 1.0959 - val_acc: 0.6155\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.8019 - acc: 0.7314 - val_loss: 1.0934 - val_acc: 0.6145\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.7749 - acc: 0.7380 - val_loss: 1.0773 - val_acc: 0.6195\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.7451 - acc: 0.7470 - val_loss: 1.0743 - val_acc: 0.6185\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.7238 - acc: 0.7575 - val_loss: 1.0686 - val_acc: 0.6235\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.7011 - acc: 0.7646 - val_loss: 1.0713 - val_acc: 0.6175\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.6821 - acc: 0.7651 - val_loss: 1.0641 - val_acc: 0.6205\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.6584 - acc: 0.7691 - val_loss: 1.0626 - val_acc: 0.6145\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.6425 - acc: 0.7711 - val_loss: 1.0651 - val_acc: 0.6185\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.6210 - acc: 0.7761 - val_loss: 1.0630 - val_acc: 0.6205\n",
      "Epoch 00026: early stopping\n",
      "996/996 [==============================] - 0s 305us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 2.0334 - acc: 0.2892 - val_loss: 1.8267 - val_acc: 0.3173\n",
      "Epoch 2/50\n",
      " - 3s - loss: 1.7916 - acc: 0.3037 - val_loss: 1.7149 - val_acc: 0.3685\n",
      "Epoch 3/50\n",
      " - 3s - loss: 1.6949 - acc: 0.3830 - val_loss: 1.6192 - val_acc: 0.4207\n",
      "Epoch 4/50\n",
      " - 3s - loss: 1.5891 - acc: 0.4463 - val_loss: 1.5241 - val_acc: 0.4679\n",
      "Epoch 5/50\n",
      " - 3s - loss: 1.4825 - acc: 0.4930 - val_loss: 1.4394 - val_acc: 0.5030\n",
      "Epoch 6/50\n",
      " - 3s - loss: 1.3913 - acc: 0.5236 - val_loss: 1.3782 - val_acc: 0.5432\n",
      "Epoch 7/50\n",
      " - 3s - loss: 1.3149 - acc: 0.5673 - val_loss: 1.3165 - val_acc: 0.5743\n",
      "Epoch 8/50\n",
      " - 3s - loss: 1.2487 - acc: 0.5959 - val_loss: 1.2691 - val_acc: 0.5843\n",
      "Epoch 9/50\n",
      " - 3s - loss: 1.1909 - acc: 0.6255 - val_loss: 1.2262 - val_acc: 0.5873\n",
      "Epoch 10/50\n",
      " - 3s - loss: 1.1391 - acc: 0.6340 - val_loss: 1.1906 - val_acc: 0.5994\n",
      "Epoch 11/50\n",
      " - 3s - loss: 1.0887 - acc: 0.6561 - val_loss: 1.1641 - val_acc: 0.6104\n",
      "Epoch 12/50\n",
      " - 3s - loss: 1.0471 - acc: 0.6601 - val_loss: 1.1406 - val_acc: 0.6024\n",
      "Epoch 13/50\n",
      " - 3s - loss: 1.0077 - acc: 0.6616 - val_loss: 1.1097 - val_acc: 0.6255\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.9697 - acc: 0.6777 - val_loss: 1.0910 - val_acc: 0.6235\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.9343 - acc: 0.6913 - val_loss: 1.0737 - val_acc: 0.6255\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.9011 - acc: 0.7013 - val_loss: 1.0587 - val_acc: 0.6195\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.8679 - acc: 0.7083 - val_loss: 1.0533 - val_acc: 0.6205\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.8394 - acc: 0.7129 - val_loss: 1.0409 - val_acc: 0.6245\n",
      "Epoch 00018: early stopping\n",
      "996/996 [==============================] - 0s 311us/step\n",
      "Training with: 1000\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 1.9574 - acc: 0.3002 - val_loss: 1.8473 - val_acc: 0.2540\n",
      "Epoch 2/50\n",
      " - 6s - loss: 1.7207 - acc: 0.3288 - val_loss: 1.7262 - val_acc: 0.3313\n",
      "Epoch 3/50\n",
      " - 6s - loss: 1.5964 - acc: 0.4182 - val_loss: 1.5972 - val_acc: 0.4137\n",
      "Epoch 4/50\n",
      " - 6s - loss: 1.4498 - acc: 0.4990 - val_loss: 1.4758 - val_acc: 0.4789\n",
      "Epoch 5/50\n",
      " - 6s - loss: 1.3277 - acc: 0.5542 - val_loss: 1.3860 - val_acc: 0.5100\n",
      "Epoch 6/50\n",
      " - 6s - loss: 1.2235 - acc: 0.5899 - val_loss: 1.3126 - val_acc: 0.5402\n",
      "Epoch 7/50\n",
      " - 6s - loss: 1.1407 - acc: 0.6260 - val_loss: 1.2709 - val_acc: 0.5542\n",
      "Epoch 8/50\n",
      " - 6s - loss: 1.0682 - acc: 0.6506 - val_loss: 1.2399 - val_acc: 0.5683\n",
      "Epoch 9/50\n",
      " - 6s - loss: 1.0027 - acc: 0.6737 - val_loss: 1.2029 - val_acc: 0.5683\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.9466 - acc: 0.6948 - val_loss: 1.1887 - val_acc: 0.5743\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.8993 - acc: 0.7058 - val_loss: 1.1774 - val_acc: 0.5783\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.8553 - acc: 0.7284 - val_loss: 1.1488 - val_acc: 0.5873\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.8128 - acc: 0.7435 - val_loss: 1.1394 - val_acc: 0.5884\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.7734 - acc: 0.7460 - val_loss: 1.1374 - val_acc: 0.5873\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.7401 - acc: 0.7505 - val_loss: 1.1253 - val_acc: 0.5793\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.7043 - acc: 0.7656 - val_loss: 1.1079 - val_acc: 0.5924\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.6735 - acc: 0.7711 - val_loss: 1.1021 - val_acc: 0.5904\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.6443 - acc: 0.7831 - val_loss: 1.0985 - val_acc: 0.5914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      " - 6s - loss: 0.6172 - acc: 0.7987 - val_loss: 1.0970 - val_acc: 0.5964\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.5918 - acc: 0.8037 - val_loss: 1.0982 - val_acc: 0.5964\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.5679 - acc: 0.8092 - val_loss: 1.0875 - val_acc: 0.6054\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.5481 - acc: 0.8153 - val_loss: 1.1031 - val_acc: 0.6074\n",
      "Epoch 23/50\n",
      " - 6s - loss: 0.5297 - acc: 0.8223 - val_loss: 1.1050 - val_acc: 0.5994\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.5133 - acc: 0.8208 - val_loss: 1.0939 - val_acc: 0.5964\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.4978 - acc: 0.8268 - val_loss: 1.1078 - val_acc: 0.6054\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.4805 - acc: 0.8278 - val_loss: 1.1231 - val_acc: 0.6044\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.4659 - acc: 0.8373 - val_loss: 1.1186 - val_acc: 0.6094\n",
      "Epoch 28/50\n",
      " - 6s - loss: 0.4562 - acc: 0.8373 - val_loss: 1.1104 - val_acc: 0.6124\n",
      "Epoch 29/50\n",
      " - 6s - loss: 0.4425 - acc: 0.8384 - val_loss: 1.1104 - val_acc: 0.6004\n",
      "Epoch 30/50\n",
      " - 6s - loss: 0.4309 - acc: 0.8459 - val_loss: 1.1177 - val_acc: 0.6145\n",
      "Epoch 31/50\n",
      " - 6s - loss: 0.4207 - acc: 0.8499 - val_loss: 1.1221 - val_acc: 0.6014\n",
      "Epoch 32/50\n",
      " - 6s - loss: 0.4103 - acc: 0.8444 - val_loss: 1.1298 - val_acc: 0.6094\n",
      "Epoch 33/50\n",
      " - 6s - loss: 0.4034 - acc: 0.8524 - val_loss: 1.1738 - val_acc: 0.6124\n",
      "Epoch 34/50\n",
      " - 6s - loss: 0.3955 - acc: 0.8514 - val_loss: 1.1402 - val_acc: 0.6054\n",
      "Epoch 35/50\n",
      " - 6s - loss: 0.3873 - acc: 0.8544 - val_loss: 1.1592 - val_acc: 0.6104\n",
      "Epoch 00035: early stopping\n",
      "996/996 [==============================] - 1s 548us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 1.9724 - acc: 0.2801 - val_loss: 1.7786 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 6s - loss: 1.7334 - acc: 0.3559 - val_loss: 1.6702 - val_acc: 0.4016\n",
      "Epoch 3/50\n",
      " - 6s - loss: 1.5905 - acc: 0.4393 - val_loss: 1.5252 - val_acc: 0.4930\n",
      "Epoch 4/50\n",
      " - 6s - loss: 1.4395 - acc: 0.5115 - val_loss: 1.4063 - val_acc: 0.5412\n",
      "Epoch 5/50\n",
      " - 6s - loss: 1.3157 - acc: 0.5612 - val_loss: 1.3158 - val_acc: 0.5412\n",
      "Epoch 6/50\n",
      " - 6s - loss: 1.2173 - acc: 0.5868 - val_loss: 1.2587 - val_acc: 0.5914\n",
      "Epoch 7/50\n",
      " - 6s - loss: 1.1361 - acc: 0.6416 - val_loss: 1.2056 - val_acc: 0.5974\n",
      "Epoch 8/50\n",
      " - 6s - loss: 1.0649 - acc: 0.6536 - val_loss: 1.1725 - val_acc: 0.6024\n",
      "Epoch 9/50\n",
      " - 6s - loss: 1.0053 - acc: 0.6727 - val_loss: 1.1443 - val_acc: 0.6004\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.9501 - acc: 0.6888 - val_loss: 1.1236 - val_acc: 0.6084\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.8983 - acc: 0.6993 - val_loss: 1.1097 - val_acc: 0.6104\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.8523 - acc: 0.7123 - val_loss: 1.0935 - val_acc: 0.6104\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.8085 - acc: 0.7329 - val_loss: 1.0857 - val_acc: 0.6155\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.7746 - acc: 0.7390 - val_loss: 1.0790 - val_acc: 0.6185\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.7381 - acc: 0.7485 - val_loss: 1.0706 - val_acc: 0.6255\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.7062 - acc: 0.7535 - val_loss: 1.0659 - val_acc: 0.6155\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.6764 - acc: 0.7696 - val_loss: 1.0633 - val_acc: 0.6175\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.6507 - acc: 0.7751 - val_loss: 1.0582 - val_acc: 0.6124\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.6248 - acc: 0.7751 - val_loss: 1.0631 - val_acc: 0.6145\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.6004 - acc: 0.7846 - val_loss: 1.0639 - val_acc: 0.6225\n",
      "Epoch 00020: early stopping\n",
      "996/996 [==============================] - 1s 546us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 1.9733 - acc: 0.2701 - val_loss: 1.7705 - val_acc: 0.3082\n",
      "Epoch 2/50\n",
      " - 6s - loss: 1.7495 - acc: 0.3198 - val_loss: 1.6575 - val_acc: 0.3765\n",
      "Epoch 3/50\n",
      " - 6s - loss: 1.6132 - acc: 0.4388 - val_loss: 1.5256 - val_acc: 0.4639\n",
      "Epoch 4/50\n",
      " - 6s - loss: 1.4710 - acc: 0.4965 - val_loss: 1.4186 - val_acc: 0.5020\n",
      "Epoch 5/50\n",
      " - 6s - loss: 1.3559 - acc: 0.5301 - val_loss: 1.3358 - val_acc: 0.5693\n",
      "Epoch 6/50\n",
      " - 6s - loss: 1.2598 - acc: 0.5833 - val_loss: 1.2634 - val_acc: 0.5723\n",
      "Epoch 7/50\n",
      " - 6s - loss: 1.1791 - acc: 0.6074 - val_loss: 1.2173 - val_acc: 0.6044\n",
      "Epoch 8/50\n",
      " - 6s - loss: 1.1126 - acc: 0.6396 - val_loss: 1.1651 - val_acc: 0.6114\n",
      "Epoch 9/50\n",
      " - 6s - loss: 1.0519 - acc: 0.6601 - val_loss: 1.1369 - val_acc: 0.6165\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.9956 - acc: 0.6747 - val_loss: 1.1004 - val_acc: 0.6195\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.9479 - acc: 0.6827 - val_loss: 1.0758 - val_acc: 0.6195\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.8991 - acc: 0.6918 - val_loss: 1.0669 - val_acc: 0.6235\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.8605 - acc: 0.7053 - val_loss: 1.0463 - val_acc: 0.6295\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.8192 - acc: 0.7239 - val_loss: 1.0240 - val_acc: 0.6325\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.7816 - acc: 0.7309 - val_loss: 1.0096 - val_acc: 0.6376\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.7463 - acc: 0.7410 - val_loss: 1.0185 - val_acc: 0.6215\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.7161 - acc: 0.7500 - val_loss: 1.0027 - val_acc: 0.6295\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.6854 - acc: 0.7570 - val_loss: 0.9851 - val_acc: 0.6295\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.6555 - acc: 0.7706 - val_loss: 0.9894 - val_acc: 0.6305\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.6296 - acc: 0.7811 - val_loss: 1.0024 - val_acc: 0.6235\n",
      "Epoch 00020: early stopping\n",
      "996/996 [==============================] - 1s 544us/step\n",
      "Training with: 1500\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9341 - acc: 0.3007 - val_loss: 1.8142 - val_acc: 0.2530\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.6908 - acc: 0.3554 - val_loss: 1.6785 - val_acc: 0.3906\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5316 - acc: 0.4558 - val_loss: 1.5244 - val_acc: 0.4498\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.3721 - acc: 0.5271 - val_loss: 1.4057 - val_acc: 0.4990\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2415 - acc: 0.5899 - val_loss: 1.3240 - val_acc: 0.5281\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1364 - acc: 0.6355 - val_loss: 1.2736 - val_acc: 0.5392\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0505 - acc: 0.6541 - val_loss: 1.2298 - val_acc: 0.5552\n",
      "Epoch 8/50\n",
      " - 9s - loss: 0.9785 - acc: 0.6822 - val_loss: 1.1902 - val_acc: 0.5693\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9170 - acc: 0.7018 - val_loss: 1.1759 - val_acc: 0.5813\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.8626 - acc: 0.7219 - val_loss: 1.1531 - val_acc: 0.5944\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8103 - acc: 0.7435 - val_loss: 1.1387 - val_acc: 0.5954\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.7707 - acc: 0.7415 - val_loss: 1.1231 - val_acc: 0.5793\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7282 - acc: 0.7595 - val_loss: 1.1083 - val_acc: 0.5944\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.6892 - acc: 0.7771 - val_loss: 1.1006 - val_acc: 0.5904\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.6560 - acc: 0.7756 - val_loss: 1.1068 - val_acc: 0.5994\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6208 - acc: 0.7932 - val_loss: 1.0941 - val_acc: 0.6044\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.5918 - acc: 0.8067 - val_loss: 1.0929 - val_acc: 0.5964\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.5706 - acc: 0.8042 - val_loss: 1.0922 - val_acc: 0.5944\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.5396 - acc: 0.8193 - val_loss: 1.0989 - val_acc: 0.6044\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.5215 - acc: 0.8188 - val_loss: 1.0908 - val_acc: 0.5994\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.4988 - acc: 0.8238 - val_loss: 1.1077 - val_acc: 0.6074\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.4852 - acc: 0.8313 - val_loss: 1.0931 - val_acc: 0.5984\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.4696 - acc: 0.8303 - val_loss: 1.1097 - val_acc: 0.6094\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.4506 - acc: 0.8394 - val_loss: 1.1225 - val_acc: 0.6135\n",
      "Epoch 25/50\n",
      " - 9s - loss: 0.4409 - acc: 0.8389 - val_loss: 1.1251 - val_acc: 0.6104\n",
      "Epoch 26/50\n",
      " - 9s - loss: 0.4273 - acc: 0.8494 - val_loss: 1.1345 - val_acc: 0.5984\n",
      "Epoch 27/50\n",
      " - 9s - loss: 0.4165 - acc: 0.8459 - val_loss: 1.1596 - val_acc: 0.6135\n",
      "Epoch 28/50\n",
      " - 9s - loss: 0.4108 - acc: 0.8464 - val_loss: 1.1304 - val_acc: 0.5934\n",
      "Epoch 29/50\n",
      " - 9s - loss: 0.3987 - acc: 0.8484 - val_loss: 1.1694 - val_acc: 0.6155\n",
      "Epoch 30/50\n",
      " - 9s - loss: 0.3843 - acc: 0.8509 - val_loss: 1.1495 - val_acc: 0.6114\n",
      "Epoch 31/50\n",
      " - 9s - loss: 0.3829 - acc: 0.8504 - val_loss: 1.1614 - val_acc: 0.6054\n",
      "Epoch 32/50\n",
      " - 9s - loss: 0.3706 - acc: 0.8554 - val_loss: 1.1611 - val_acc: 0.6084\n",
      "Epoch 33/50\n",
      " - 9s - loss: 0.3642 - acc: 0.8524 - val_loss: 1.2046 - val_acc: 0.6124\n",
      "Epoch 34/50\n",
      " - 9s - loss: 0.3582 - acc: 0.8624 - val_loss: 1.1945 - val_acc: 0.6104\n",
      "Epoch 00034: early stopping\n",
      "996/996 [==============================] - 1s 814us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9543 - acc: 0.2917 - val_loss: 1.7618 - val_acc: 0.3012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      " - 9s - loss: 1.7025 - acc: 0.3589 - val_loss: 1.6124 - val_acc: 0.4508\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5273 - acc: 0.4749 - val_loss: 1.4575 - val_acc: 0.5151\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.3597 - acc: 0.5467 - val_loss: 1.3381 - val_acc: 0.5422\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2351 - acc: 0.5914 - val_loss: 1.2575 - val_acc: 0.5542\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1350 - acc: 0.6340 - val_loss: 1.1993 - val_acc: 0.5733\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0531 - acc: 0.6511 - val_loss: 1.1605 - val_acc: 0.6034\n",
      "Epoch 8/50\n",
      " - 9s - loss: 0.9810 - acc: 0.6742 - val_loss: 1.1368 - val_acc: 0.5984\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9176 - acc: 0.6928 - val_loss: 1.1102 - val_acc: 0.6104\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.8643 - acc: 0.7108 - val_loss: 1.0992 - val_acc: 0.6145\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8179 - acc: 0.7249 - val_loss: 1.0861 - val_acc: 0.6135\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.7674 - acc: 0.7359 - val_loss: 1.0710 - val_acc: 0.6205\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7275 - acc: 0.7550 - val_loss: 1.0681 - val_acc: 0.6215\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.6913 - acc: 0.7666 - val_loss: 1.0676 - val_acc: 0.6225\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.6560 - acc: 0.7711 - val_loss: 1.0674 - val_acc: 0.6165\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6292 - acc: 0.7786 - val_loss: 1.0724 - val_acc: 0.6175\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6000 - acc: 0.7846 - val_loss: 1.0642 - val_acc: 0.6135\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.5743 - acc: 0.7922 - val_loss: 1.0647 - val_acc: 0.6275\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.5534 - acc: 0.8012 - val_loss: 1.0642 - val_acc: 0.6114\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.5330 - acc: 0.8072 - val_loss: 1.0802 - val_acc: 0.6084\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.5142 - acc: 0.8072 - val_loss: 1.0707 - val_acc: 0.6124\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.4974 - acc: 0.8183 - val_loss: 1.0787 - val_acc: 0.6024\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.4830 - acc: 0.8233 - val_loss: 1.0804 - val_acc: 0.6265\n",
      "Epoch 00023: early stopping\n",
      "996/996 [==============================] - 1s 815us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9543 - acc: 0.2726 - val_loss: 1.7542 - val_acc: 0.3253\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7191 - acc: 0.3559 - val_loss: 1.6251 - val_acc: 0.4639\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5566 - acc: 0.4684 - val_loss: 1.4721 - val_acc: 0.4890\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4053 - acc: 0.5110 - val_loss: 1.3576 - val_acc: 0.5432\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2875 - acc: 0.5718 - val_loss: 1.2796 - val_acc: 0.5733\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1924 - acc: 0.6104 - val_loss: 1.2065 - val_acc: 0.5904\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.1100 - acc: 0.6386 - val_loss: 1.1591 - val_acc: 0.6094\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0441 - acc: 0.6571 - val_loss: 1.1262 - val_acc: 0.6104\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9815 - acc: 0.6767 - val_loss: 1.0878 - val_acc: 0.6225\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.9236 - acc: 0.6883 - val_loss: 1.0579 - val_acc: 0.6335\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8725 - acc: 0.7033 - val_loss: 1.0424 - val_acc: 0.6416\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.8345 - acc: 0.7118 - val_loss: 1.0224 - val_acc: 0.6355\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7823 - acc: 0.7299 - val_loss: 1.0153 - val_acc: 0.6305\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7455 - acc: 0.7430 - val_loss: 0.9959 - val_acc: 0.6396\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.7059 - acc: 0.7515 - val_loss: 1.0083 - val_acc: 0.6295\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6713 - acc: 0.7615 - val_loss: 0.9875 - val_acc: 0.6376\n",
      "Epoch 00016: early stopping\n",
      "996/996 [==============================] - 1s 817us/step\n",
      "Training with: 2000\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.9105 - acc: 0.2977 - val_loss: 1.8036 - val_acc: 0.2540\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.6635 - acc: 0.3695 - val_loss: 1.6519 - val_acc: 0.3624\n",
      "Epoch 3/50\n",
      " - 11s - loss: 1.4781 - acc: 0.4910 - val_loss: 1.4850 - val_acc: 0.4528\n",
      "Epoch 4/50\n",
      " - 11s - loss: 1.3169 - acc: 0.5492 - val_loss: 1.3642 - val_acc: 0.5070\n",
      "Epoch 5/50\n",
      " - 11s - loss: 1.1937 - acc: 0.6019 - val_loss: 1.2868 - val_acc: 0.5492\n",
      "Epoch 6/50\n",
      " - 11s - loss: 1.0976 - acc: 0.6386 - val_loss: 1.2580 - val_acc: 0.5422\n",
      "Epoch 7/50\n",
      " - 11s - loss: 1.0084 - acc: 0.6652 - val_loss: 1.1958 - val_acc: 0.5612\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.9420 - acc: 0.6883 - val_loss: 1.1818 - val_acc: 0.5783\n",
      "Epoch 9/50\n",
      " - 11s - loss: 0.8766 - acc: 0.7129 - val_loss: 1.1655 - val_acc: 0.5763\n",
      "Epoch 10/50\n",
      " - 11s - loss: 0.8198 - acc: 0.7354 - val_loss: 1.1476 - val_acc: 0.5783\n",
      "Epoch 11/50\n",
      " - 11s - loss: 0.7691 - acc: 0.7470 - val_loss: 1.1151 - val_acc: 0.5964\n",
      "Epoch 12/50\n",
      " - 11s - loss: 0.7227 - acc: 0.7600 - val_loss: 1.1011 - val_acc: 0.5954\n",
      "Epoch 13/50\n",
      " - 11s - loss: 0.6806 - acc: 0.7721 - val_loss: 1.1090 - val_acc: 0.5914\n",
      "Epoch 14/50\n",
      " - 11s - loss: 0.6417 - acc: 0.7851 - val_loss: 1.0946 - val_acc: 0.6024\n",
      "Epoch 15/50\n",
      " - 11s - loss: 0.6065 - acc: 0.7917 - val_loss: 1.1232 - val_acc: 0.5954\n",
      "Epoch 16/50\n",
      " - 11s - loss: 0.5750 - acc: 0.8027 - val_loss: 1.0992 - val_acc: 0.5904\n",
      "Epoch 17/50\n",
      " - 11s - loss: 0.5510 - acc: 0.8102 - val_loss: 1.0908 - val_acc: 0.6044\n",
      "Epoch 18/50\n",
      " - 11s - loss: 0.5227 - acc: 0.8228 - val_loss: 1.1149 - val_acc: 0.6014\n",
      "Epoch 19/50\n",
      " - 11s - loss: 0.4988 - acc: 0.8238 - val_loss: 1.0868 - val_acc: 0.6135\n",
      "Epoch 20/50\n",
      " - 11s - loss: 0.4844 - acc: 0.8283 - val_loss: 1.1171 - val_acc: 0.6084\n",
      "Epoch 21/50\n",
      " - 11s - loss: 0.4615 - acc: 0.8353 - val_loss: 1.0976 - val_acc: 0.6034\n",
      "Epoch 22/50\n",
      " - 11s - loss: 0.4468 - acc: 0.8414 - val_loss: 1.1554 - val_acc: 0.6094\n",
      "Epoch 23/50\n",
      " - 11s - loss: 0.4327 - acc: 0.8419 - val_loss: 1.1142 - val_acc: 0.6034\n",
      "Epoch 24/50\n",
      " - 11s - loss: 0.4203 - acc: 0.8444 - val_loss: 1.1402 - val_acc: 0.6034\n",
      "Epoch 00024: early stopping\n",
      "996/996 [==============================] - 1s 926us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.9332 - acc: 0.2786 - val_loss: 1.7440 - val_acc: 0.3624\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.6747 - acc: 0.3775 - val_loss: 1.5853 - val_acc: 0.4508\n",
      "Epoch 3/50\n",
      " - 11s - loss: 1.4914 - acc: 0.4764 - val_loss: 1.4220 - val_acc: 0.5161\n",
      "Epoch 4/50\n",
      " - 11s - loss: 1.3275 - acc: 0.5648 - val_loss: 1.3084 - val_acc: 0.5371\n",
      "Epoch 5/50\n",
      " - 11s - loss: 1.1997 - acc: 0.5989 - val_loss: 1.2322 - val_acc: 0.5512\n",
      "Epoch 6/50\n",
      " - 11s - loss: 1.1001 - acc: 0.6391 - val_loss: 1.1842 - val_acc: 0.5914\n",
      "Epoch 7/50\n",
      " - 11s - loss: 1.0215 - acc: 0.6616 - val_loss: 1.1436 - val_acc: 0.5974\n",
      "Epoch 8/50\n",
      " - 11s - loss: 0.9476 - acc: 0.6867 - val_loss: 1.1197 - val_acc: 0.6104\n",
      "Epoch 9/50\n",
      " - 11s - loss: 0.8809 - acc: 0.7068 - val_loss: 1.1000 - val_acc: 0.6145\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.8238 - acc: 0.7229 - val_loss: 1.0859 - val_acc: 0.6225\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.7746 - acc: 0.7369 - val_loss: 1.0770 - val_acc: 0.6265\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.7288 - acc: 0.7510 - val_loss: 1.0721 - val_acc: 0.6205\n",
      "Epoch 13/50\n",
      " - 11s - loss: 0.6917 - acc: 0.7525 - val_loss: 1.0703 - val_acc: 0.6205\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.6559 - acc: 0.7716 - val_loss: 1.0638 - val_acc: 0.6205\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.6198 - acc: 0.7746 - val_loss: 1.0638 - val_acc: 0.6185\n",
      "Epoch 16/50\n",
      " - 11s - loss: 0.5929 - acc: 0.7836 - val_loss: 1.0675 - val_acc: 0.6135\n",
      "Epoch 00016: early stopping\n",
      "996/996 [==============================] - 1s 929us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.9318 - acc: 0.3042 - val_loss: 1.7367 - val_acc: 0.3263\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.6858 - acc: 0.3725 - val_loss: 1.5695 - val_acc: 0.4257\n",
      "Epoch 3/50\n",
      " - 11s - loss: 1.5000 - acc: 0.4699 - val_loss: 1.4171 - val_acc: 0.5040\n",
      "Epoch 4/50\n",
      " - 11s - loss: 1.3418 - acc: 0.5397 - val_loss: 1.3191 - val_acc: 0.5944\n",
      "Epoch 5/50\n",
      " - 11s - loss: 1.2258 - acc: 0.5873 - val_loss: 1.2435 - val_acc: 0.5833\n",
      "Epoch 6/50\n",
      " - 11s - loss: 1.1331 - acc: 0.6165 - val_loss: 1.1666 - val_acc: 0.6104\n",
      "Epoch 7/50\n",
      " - 11s - loss: 1.0504 - acc: 0.6526 - val_loss: 1.1165 - val_acc: 0.6245\n",
      "Epoch 8/50\n",
      " - 11s - loss: 0.9797 - acc: 0.6682 - val_loss: 1.0774 - val_acc: 0.6275\n",
      "Epoch 9/50\n",
      " - 11s - loss: 0.9158 - acc: 0.6903 - val_loss: 1.0559 - val_acc: 0.6325\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.8566 - acc: 0.6998 - val_loss: 1.0295 - val_acc: 0.6285\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.8096 - acc: 0.7224 - val_loss: 1.0192 - val_acc: 0.6235\n",
      "Epoch 12/50\n",
      " - 11s - loss: 0.7558 - acc: 0.7364 - val_loss: 1.0073 - val_acc: 0.6335\n",
      "Epoch 13/50\n",
      " - 11s - loss: 0.7113 - acc: 0.7530 - val_loss: 1.0032 - val_acc: 0.6315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      " - 11s - loss: 0.6768 - acc: 0.7646 - val_loss: 0.9892 - val_acc: 0.6235\n",
      "Epoch 15/50\n",
      " - 11s - loss: 0.6409 - acc: 0.7716 - val_loss: 1.0047 - val_acc: 0.6265\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.6093 - acc: 0.7851 - val_loss: 0.9781 - val_acc: 0.6396\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.5811 - acc: 0.7952 - val_loss: 0.9968 - val_acc: 0.6305\n",
      "Epoch 18/50\n",
      " - 12s - loss: 0.5530 - acc: 0.8007 - val_loss: 0.9785 - val_acc: 0.6406\n",
      "Epoch 19/50\n",
      " - 12s - loss: 0.5293 - acc: 0.8087 - val_loss: 0.9908 - val_acc: 0.6406\n",
      "Epoch 20/50\n",
      " - 12s - loss: 0.5099 - acc: 0.8148 - val_loss: 1.0036 - val_acc: 0.6396\n",
      "Epoch 21/50\n",
      " - 11s - loss: 0.4889 - acc: 0.8243 - val_loss: 0.9945 - val_acc: 0.6456\n",
      "Epoch 22/50\n",
      " - 11s - loss: 0.4732 - acc: 0.8268 - val_loss: 1.0189 - val_acc: 0.6145\n",
      "Epoch 23/50\n",
      " - 11s - loss: 0.4601 - acc: 0.8303 - val_loss: 1.0151 - val_acc: 0.6345\n",
      "Epoch 24/50\n",
      " - 11s - loss: 0.4468 - acc: 0.8358 - val_loss: 1.0372 - val_acc: 0.6255\n",
      "Epoch 25/50\n",
      " - 11s - loss: 0.4349 - acc: 0.8394 - val_loss: 1.0307 - val_acc: 0.6205\n",
      "Epoch 26/50\n",
      " - 11s - loss: 0.4223 - acc: 0.8459 - val_loss: 1.0388 - val_acc: 0.6245\n",
      "Epoch 00026: early stopping\n",
      "996/996 [==============================] - 1s 932us/step\n"
     ]
    }
   ],
   "source": [
    "# Create a model with a single hidden layer\n",
    "def hidden_1(neurons=100):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return (model)\n",
    "\n",
    "# Setting the input params\n",
    "neurons = [500, 1000, 1500, 2000] # Number of neurons to test in the first hidden layer\n",
    "folds = 3 # Number of k-folds\n",
    "seed = 1986 # Random seed for reproducibility\n",
    "\n",
    "# Training the model\n",
    "gs1 = KerasClassifierGridSearchCV(neurons, folds, seed)\n",
    "gs1.fit(hidden_1, x_train, y_train, epochs=50, batch_size=50, early_stopping_rounds=5, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready to be trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the overall CV results. The results are displayed as **Accuracy**, so a higher number indicates better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: 0.6147925033467202,\n",
       " 1000: 0.61880856760374836,\n",
       " 1500: 0.6248326639892906,\n",
       " 2000: 0.61378848728246316}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.get_avg_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 1000 neuron architecture is best, let's look at the individual scores from n-fold. The first n-fold performs the best, so let's use that pre-trained model and evaluate it against the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: [0.5993975903614458, 0.62048192771084343, 0.62449799196787148],\n",
       " 1000: [0.61044176706827313, 0.6224899598393574, 0.62349397590361444],\n",
       " 1500: [0.61044176706827313, 0.62650602409638556, 0.6375502008032129],\n",
       " 2000: [0.60341365461847385, 0.61345381526104414, 0.62449799196787148]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to re-train the model from scratch. So get the best model and weights from the grid search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Get the best model from the 1000 neuron n-folds\n",
    "best_model = gs1.get_models()[1500][2]\n",
    "\n",
    "# Save the best model in case we want to reference it later\n",
    "best_model.save('models/iteration1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the entire dataset. Use checkpoints to save the best iteration. This is usefull if the best iteration isn't the final epoch, which is likely to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/50\n",
      " - 40s - loss: 0.5333 - acc: 0.7999 - val_loss: 1.0141 - val_acc: 0.6456\n",
      "Epoch 2/50\n",
      " - 41s - loss: 0.5122 - acc: 0.8025 - val_loss: 1.0263 - val_acc: 0.6517\n",
      "Epoch 3/50\n",
      " - 40s - loss: 0.4918 - acc: 0.8046 - val_loss: 1.0227 - val_acc: 0.6757\n",
      "Epoch 4/50\n",
      " - 20s - loss: 0.4722 - acc: 0.8143 - val_loss: 1.0296 - val_acc: 0.6607\n",
      "Epoch 5/50\n",
      " - 20s - loss: 0.4644 - acc: 0.8149 - val_loss: 1.0132 - val_acc: 0.6757\n",
      "Epoch 6/50\n",
      " - 39s - loss: 0.4486 - acc: 0.8233 - val_loss: 1.0207 - val_acc: 0.6817\n",
      "Epoch 7/50\n",
      " - 20s - loss: 0.4356 - acc: 0.8230 - val_loss: 1.0509 - val_acc: 0.6787\n",
      "Epoch 8/50\n",
      " - 20s - loss: 0.4269 - acc: 0.8246 - val_loss: 1.1046 - val_acc: 0.6727\n",
      "Epoch 9/50\n",
      " - 20s - loss: 0.4155 - acc: 0.8280 - val_loss: 1.0309 - val_acc: 0.6637\n",
      "Epoch 10/50\n",
      " - 20s - loss: 0.4058 - acc: 0.8313 - val_loss: 1.0769 - val_acc: 0.6667\n",
      "Epoch 11/50\n",
      " - 20s - loss: 0.4003 - acc: 0.8290 - val_loss: 1.0970 - val_acc: 0.6787\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6177a85f8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=5, verbose=True)\n",
    "\n",
    "# Checkpoint - used to get the best weights during the model training process\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_weights.h5', monitor='val_acc', save_best_only=True)\n",
    "\n",
    "# Fit the pre-trained best model on the entire dataset\n",
    "best_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping, checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make the predictions and evaluate the confusion matrix. First we'll define a helper function for plotting a confusion matrix. Then we'll make the predictions and do the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEmCAYAAAD1FIKpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa61777b4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best iteration\n",
    "best_model = load_model('models/best_weights.h5')\n",
    "\n",
    "# Make the predictions\n",
    "preds = best_model.predict_classes(x_test)\n",
    "preds = [i + 1 for i in preds]\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test_true, preds, labels=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[1,2,3,4,5,6,7,8,9], title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68168\n",
      "333/333 [==============================] - 1s 2ms/step\n",
      "[1.0207363470896587, 0.68168168221866043]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "acc = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print('Accuracy: %0.5f' % acc)\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in case we want to reference it later\n",
    "best_model.save('models/iteration1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if we can get even better performance if we are to add another hidden layer to the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: 0\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.9026 - acc: 0.3032 - val_loss: 1.8133 - val_acc: 0.2530\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.6510 - acc: 0.3956 - val_loss: 1.6321 - val_acc: 0.3825\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.4608 - acc: 0.4739 - val_loss: 1.4565 - val_acc: 0.4910\n",
      "Epoch 4/50\n",
      " - 11s - loss: 1.3022 - acc: 0.5547 - val_loss: 1.3478 - val_acc: 0.5351\n",
      "Epoch 5/50\n",
      " - 11s - loss: 1.1761 - acc: 0.6069 - val_loss: 1.2769 - val_acc: 0.5713\n",
      "Epoch 6/50\n",
      " - 11s - loss: 1.0812 - acc: 0.6416 - val_loss: 1.2282 - val_acc: 0.5622\n",
      "Epoch 7/50\n",
      " - 11s - loss: 1.0013 - acc: 0.6792 - val_loss: 1.1915 - val_acc: 0.5663\n",
      "Epoch 8/50\n",
      " - 11s - loss: 0.9278 - acc: 0.7003 - val_loss: 1.1669 - val_acc: 0.5833\n",
      "Epoch 9/50\n",
      " - 11s - loss: 0.8667 - acc: 0.7129 - val_loss: 1.1415 - val_acc: 0.5793\n",
      "Epoch 10/50\n",
      " - 11s - loss: 0.8091 - acc: 0.7440 - val_loss: 1.1283 - val_acc: 0.5773\n",
      "Epoch 11/50\n",
      " - 11s - loss: 0.7653 - acc: 0.7445 - val_loss: 1.1232 - val_acc: 0.5783\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.7134 - acc: 0.7641 - val_loss: 1.1042 - val_acc: 0.5783\n",
      "Epoch 13/50\n",
      " - 11s - loss: 0.6732 - acc: 0.7736 - val_loss: 1.0960 - val_acc: 0.5944\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.6350 - acc: 0.7846 - val_loss: 1.0970 - val_acc: 0.5803\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.6029 - acc: 0.7977 - val_loss: 1.0933 - val_acc: 0.5914\n",
      "Epoch 16/50\n",
      " - 11s - loss: 0.5708 - acc: 0.8027 - val_loss: 1.0877 - val_acc: 0.6104\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.5411 - acc: 0.8138 - val_loss: 1.0783 - val_acc: 0.6014\n",
      "Epoch 18/50\n",
      " - 11s - loss: 0.5197 - acc: 0.8158 - val_loss: 1.0975 - val_acc: 0.5924\n",
      "Epoch 19/50\n",
      " - 11s - loss: 0.4959 - acc: 0.8208 - val_loss: 1.1025 - val_acc: 0.5964\n",
      "Epoch 20/50\n",
      " - 11s - loss: 0.4725 - acc: 0.8379 - val_loss: 1.0909 - val_acc: 0.6064\n",
      "Epoch 21/50\n",
      " - 12s - loss: 0.4662 - acc: 0.8343 - val_loss: 1.1215 - val_acc: 0.6145\n",
      "Epoch 22/50\n",
      " - 11s - loss: 0.4449 - acc: 0.8353 - val_loss: 1.1204 - val_acc: 0.6155\n",
      "Epoch 23/50\n",
      " - 11s - loss: 0.4325 - acc: 0.8358 - val_loss: 1.1329 - val_acc: 0.6124\n",
      "Epoch 24/50\n",
      " - 11s - loss: 0.4188 - acc: 0.8444 - val_loss: 1.1442 - val_acc: 0.6094\n",
      "Epoch 25/50\n",
      " - 12s - loss: 0.4062 - acc: 0.8429 - val_loss: 1.1302 - val_acc: 0.5994\n",
      "Epoch 26/50\n",
      " - 11s - loss: 0.3954 - acc: 0.8529 - val_loss: 1.1827 - val_acc: 0.6245\n",
      "Epoch 27/50\n",
      " - 11s - loss: 0.3858 - acc: 0.8529 - val_loss: 1.1740 - val_acc: 0.6135\n",
      "Epoch 28/50\n",
      " - 11s - loss: 0.3822 - acc: 0.8494 - val_loss: 1.1746 - val_acc: 0.6094\n",
      "Epoch 29/50\n",
      " - 12s - loss: 0.3683 - acc: 0.8559 - val_loss: 1.2030 - val_acc: 0.6124\n",
      "Epoch 30/50\n",
      " - 11s - loss: 0.3628 - acc: 0.8534 - val_loss: 1.1913 - val_acc: 0.6024\n",
      "Epoch 31/50\n",
      " - 11s - loss: 0.3558 - acc: 0.8569 - val_loss: 1.2187 - val_acc: 0.6135\n",
      "Epoch 00031: early stopping\n",
      "996/996 [==============================] - 1s 932us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.9324 - acc: 0.2821 - val_loss: 1.7463 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.6764 - acc: 0.3619 - val_loss: 1.5846 - val_acc: 0.4689\n",
      "Epoch 3/50\n",
      " - 11s - loss: 1.4886 - acc: 0.4849 - val_loss: 1.4226 - val_acc: 0.5191\n",
      "Epoch 4/50\n",
      " - 11s - loss: 1.3263 - acc: 0.5587 - val_loss: 1.3225 - val_acc: 0.5402\n",
      "Epoch 5/50\n",
      " - 11s - loss: 1.2019 - acc: 0.6054 - val_loss: 1.2327 - val_acc: 0.5763\n",
      "Epoch 6/50\n",
      " - 11s - loss: 1.1005 - acc: 0.6421 - val_loss: 1.1825 - val_acc: 0.5994\n",
      "Epoch 7/50\n",
      " - 11s - loss: 1.0180 - acc: 0.6591 - val_loss: 1.1469 - val_acc: 0.5964\n",
      "Epoch 8/50\n",
      " - 11s - loss: 0.9429 - acc: 0.6903 - val_loss: 1.1282 - val_acc: 0.6014\n",
      "Epoch 9/50\n",
      " - 11s - loss: 0.8845 - acc: 0.7073 - val_loss: 1.1111 - val_acc: 0.6084\n",
      "Epoch 10/50\n",
      " - 11s - loss: 0.8272 - acc: 0.7194 - val_loss: 1.0884 - val_acc: 0.6074\n",
      "Epoch 11/50\n",
      " - 11s - loss: 0.7758 - acc: 0.7415 - val_loss: 1.0759 - val_acc: 0.6215\n",
      "Epoch 12/50\n",
      " - 11s - loss: 0.7341 - acc: 0.7410 - val_loss: 1.0686 - val_acc: 0.6165\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.6958 - acc: 0.7605 - val_loss: 1.0598 - val_acc: 0.6215\n",
      "Epoch 14/50\n",
      " - 11s - loss: 0.6528 - acc: 0.7696 - val_loss: 1.0646 - val_acc: 0.6145\n",
      "Epoch 15/50\n",
      " - 11s - loss: 0.6243 - acc: 0.7731 - val_loss: 1.0663 - val_acc: 0.6114\n",
      "Epoch 16/50\n",
      " - 11s - loss: 0.5917 - acc: 0.7892 - val_loss: 1.0713 - val_acc: 0.6275\n",
      "Epoch 17/50\n",
      " - 11s - loss: 0.5638 - acc: 0.7877 - val_loss: 1.0632 - val_acc: 0.6124\n",
      "Epoch 18/50\n",
      " - 11s - loss: 0.5398 - acc: 0.7967 - val_loss: 1.0657 - val_acc: 0.6195\n",
      "Epoch 19/50\n",
      " - 11s - loss: 0.5179 - acc: 0.8102 - val_loss: 1.0709 - val_acc: 0.6135\n",
      "Epoch 20/50\n",
      " - 11s - loss: 0.4967 - acc: 0.8117 - val_loss: 1.0722 - val_acc: 0.6245\n",
      "Epoch 21/50\n",
      " - 11s - loss: 0.4797 - acc: 0.8183 - val_loss: 1.0763 - val_acc: 0.6104\n",
      "Epoch 00021: early stopping\n",
      "996/996 [==============================] - 1s 932us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.9325 - acc: 0.2831 - val_loss: 1.7282 - val_acc: 0.3253\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.6838 - acc: 0.3750 - val_loss: 1.5746 - val_acc: 0.4618\n",
      "Epoch 3/50\n",
      " - 11s - loss: 1.4935 - acc: 0.4829 - val_loss: 1.4084 - val_acc: 0.5221\n",
      "Epoch 4/50\n",
      " - 11s - loss: 1.3315 - acc: 0.5547 - val_loss: 1.2988 - val_acc: 0.5633\n",
      "Epoch 5/50\n",
      " - 11s - loss: 1.2154 - acc: 0.5924 - val_loss: 1.2279 - val_acc: 0.6034\n",
      "Epoch 6/50\n",
      " - 11s - loss: 1.1178 - acc: 0.6350 - val_loss: 1.1619 - val_acc: 0.6155\n",
      "Epoch 7/50\n",
      " - 11s - loss: 1.0460 - acc: 0.6421 - val_loss: 1.1161 - val_acc: 0.6145\n",
      "Epoch 8/50\n",
      " - 11s - loss: 0.9712 - acc: 0.6687 - val_loss: 1.0828 - val_acc: 0.6175\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.9095 - acc: 0.6878 - val_loss: 1.0615 - val_acc: 0.6205\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.8550 - acc: 0.7003 - val_loss: 1.0286 - val_acc: 0.6355\n",
      "Epoch 11/50\n",
      " - 11s - loss: 0.8027 - acc: 0.7189 - val_loss: 1.0105 - val_acc: 0.6315\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.7565 - acc: 0.7294 - val_loss: 1.0030 - val_acc: 0.6325\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.7117 - acc: 0.7550 - val_loss: 0.9850 - val_acc: 0.6365\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.6746 - acc: 0.7671 - val_loss: 0.9971 - val_acc: 0.6295\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.6411 - acc: 0.7696 - val_loss: 0.9803 - val_acc: 0.6446\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.6066 - acc: 0.7856 - val_loss: 0.9774 - val_acc: 0.6355\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.5788 - acc: 0.7947 - val_loss: 0.9830 - val_acc: 0.6365\n",
      "Epoch 18/50\n",
      " - 11s - loss: 0.5596 - acc: 0.8047 - val_loss: 0.9862 - val_acc: 0.6305\n",
      "Epoch 19/50\n",
      " - 12s - loss: 0.5343 - acc: 0.8067 - val_loss: 0.9990 - val_acc: 0.6416\n",
      "Epoch 20/50\n",
      " - 11s - loss: 0.5140 - acc: 0.8087 - val_loss: 0.9990 - val_acc: 0.6355\n",
      "Epoch 00020: early stopping\n",
      "996/996 [==============================] - 1s 930us/step\n",
      "Training with: 250\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8629 - acc: 0.3037 - val_loss: 1.7486 - val_acc: 0.3082\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.5274 - acc: 0.4403 - val_loss: 1.4769 - val_acc: 0.4247\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.2773 - acc: 0.5306 - val_loss: 1.3183 - val_acc: 0.5110\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.0896 - acc: 0.6255 - val_loss: 1.2359 - val_acc: 0.5532\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.9331 - acc: 0.6747 - val_loss: 1.1895 - val_acc: 0.5592\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.8261 - acc: 0.7199 - val_loss: 1.1480 - val_acc: 0.5833\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.7293 - acc: 0.7555 - val_loss: 1.1431 - val_acc: 0.5994\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.6444 - acc: 0.7706 - val_loss: 1.1800 - val_acc: 0.5884\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.5788 - acc: 0.7982 - val_loss: 1.1910 - val_acc: 0.5813\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.5462 - acc: 0.8002 - val_loss: 1.1692 - val_acc: 0.6034\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4971 - acc: 0.8218 - val_loss: 1.2298 - val_acc: 0.5924\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.4660 - acc: 0.8293 - val_loss: 1.2034 - val_acc: 0.5964\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4337 - acc: 0.8328 - val_loss: 1.2492 - val_acc: 0.5773\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.4417 - acc: 0.8358 - val_loss: 1.3670 - val_acc: 0.6124\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.4035 - acc: 0.8338 - val_loss: 1.2991 - val_acc: 0.6104\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.3847 - acc: 0.8449 - val_loss: 1.3696 - val_acc: 0.6084\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.3730 - acc: 0.8524 - val_loss: 1.3208 - val_acc: 0.6064\n",
      "Epoch 18/50\n",
      " - 12s - loss: 0.3605 - acc: 0.8424 - val_loss: 1.3733 - val_acc: 0.6135\n",
      "Epoch 19/50\n",
      " - 12s - loss: 0.3499 - acc: 0.8559 - val_loss: 1.4863 - val_acc: 0.6114\n",
      "Epoch 20/50\n",
      " - 12s - loss: 0.3401 - acc: 0.8584 - val_loss: 1.4040 - val_acc: 0.6084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      " - 12s - loss: 0.3447 - acc: 0.8524 - val_loss: 1.4855 - val_acc: 0.6135\n",
      "Epoch 22/50\n",
      " - 12s - loss: 0.3326 - acc: 0.8630 - val_loss: 1.4511 - val_acc: 0.6145\n",
      "Epoch 23/50\n",
      " - 12s - loss: 0.3349 - acc: 0.8614 - val_loss: 1.5111 - val_acc: 0.6175\n",
      "Epoch 24/50\n",
      " - 12s - loss: 0.3271 - acc: 0.8665 - val_loss: 1.5062 - val_acc: 0.6124\n",
      "Epoch 25/50\n",
      " - 12s - loss: 0.3215 - acc: 0.8554 - val_loss: 1.5842 - val_acc: 0.6165\n",
      "Epoch 26/50\n",
      " - 12s - loss: 0.3159 - acc: 0.8624 - val_loss: 1.5586 - val_acc: 0.6054\n",
      "Epoch 27/50\n",
      " - 12s - loss: 0.3007 - acc: 0.8665 - val_loss: 1.5026 - val_acc: 0.6024\n",
      "Epoch 28/50\n",
      " - 12s - loss: 0.3051 - acc: 0.8630 - val_loss: 1.5341 - val_acc: 0.6054\n",
      "Epoch 00028: early stopping\n",
      "996/996 [==============================] - 1s 944us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8772 - acc: 0.3047 - val_loss: 1.6771 - val_acc: 0.3886\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.5291 - acc: 0.4327 - val_loss: 1.3921 - val_acc: 0.4880\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.2780 - acc: 0.5432 - val_loss: 1.2520 - val_acc: 0.5713\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.0934 - acc: 0.6170 - val_loss: 1.1676 - val_acc: 0.6054\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.9452 - acc: 0.6737 - val_loss: 1.1131 - val_acc: 0.6155\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.8249 - acc: 0.7088 - val_loss: 1.1462 - val_acc: 0.6054\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.7367 - acc: 0.7329 - val_loss: 1.1105 - val_acc: 0.6185\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.6694 - acc: 0.7610 - val_loss: 1.1203 - val_acc: 0.6064\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.6118 - acc: 0.7736 - val_loss: 1.1475 - val_acc: 0.6255\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.5497 - acc: 0.7957 - val_loss: 1.1676 - val_acc: 0.6024\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.5078 - acc: 0.8062 - val_loss: 1.1971 - val_acc: 0.6165\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.4824 - acc: 0.8138 - val_loss: 1.2450 - val_acc: 0.5753\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4686 - acc: 0.8143 - val_loss: 1.2772 - val_acc: 0.5783\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.4437 - acc: 0.8133 - val_loss: 1.2624 - val_acc: 0.6094\n",
      "Epoch 00014: early stopping\n",
      "996/996 [==============================] - 1s 947us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8891 - acc: 0.3117 - val_loss: 1.6747 - val_acc: 0.3444\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.5568 - acc: 0.4312 - val_loss: 1.4364 - val_acc: 0.4829\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.3272 - acc: 0.5241 - val_loss: 1.2804 - val_acc: 0.5462\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.1643 - acc: 0.5848 - val_loss: 1.1930 - val_acc: 0.6074\n",
      "Epoch 5/50\n",
      " - 12s - loss: 1.0264 - acc: 0.6381 - val_loss: 1.1179 - val_acc: 0.6044\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.9150 - acc: 0.6862 - val_loss: 1.1051 - val_acc: 0.6064\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.8278 - acc: 0.7164 - val_loss: 1.0568 - val_acc: 0.6235\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.7404 - acc: 0.7339 - val_loss: 1.0583 - val_acc: 0.6305\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.6715 - acc: 0.7545 - val_loss: 1.0710 - val_acc: 0.6175\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.6041 - acc: 0.7806 - val_loss: 1.0709 - val_acc: 0.6225\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.5636 - acc: 0.7796 - val_loss: 1.0804 - val_acc: 0.6446\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.5234 - acc: 0.8057 - val_loss: 1.1377 - val_acc: 0.6396\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4873 - acc: 0.8082 - val_loss: 1.2470 - val_acc: 0.6285\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.4681 - acc: 0.8218 - val_loss: 1.1670 - val_acc: 0.6185\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.4551 - acc: 0.8293 - val_loss: 1.1631 - val_acc: 0.6325\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.4226 - acc: 0.8293 - val_loss: 1.2129 - val_acc: 0.6365\n",
      "Epoch 00016: early stopping\n",
      "996/996 [==============================] - 1s 950us/step\n",
      "Training with: 500\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8633 - acc: 0.2997 - val_loss: 1.7647 - val_acc: 0.3363\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.5399 - acc: 0.4438 - val_loss: 1.4640 - val_acc: 0.4177\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.2446 - acc: 0.5241 - val_loss: 1.3226 - val_acc: 0.5151\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.0500 - acc: 0.6350 - val_loss: 1.2354 - val_acc: 0.5602\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.8904 - acc: 0.6968 - val_loss: 1.1769 - val_acc: 0.5894\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.7572 - acc: 0.7520 - val_loss: 1.1638 - val_acc: 0.5803\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.6609 - acc: 0.7671 - val_loss: 1.1700 - val_acc: 0.5924\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.5762 - acc: 0.7972 - val_loss: 1.2095 - val_acc: 0.5863\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.5255 - acc: 0.8133 - val_loss: 1.3165 - val_acc: 0.5924\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.4888 - acc: 0.8148 - val_loss: 1.2537 - val_acc: 0.5873\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4612 - acc: 0.8358 - val_loss: 1.2843 - val_acc: 0.6014\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.4303 - acc: 0.8389 - val_loss: 1.3613 - val_acc: 0.6044\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4053 - acc: 0.8373 - val_loss: 1.3051 - val_acc: 0.6034\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.3814 - acc: 0.8454 - val_loss: 1.4296 - val_acc: 0.6135\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.3688 - acc: 0.8554 - val_loss: 1.4542 - val_acc: 0.6175\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.3562 - acc: 0.8509 - val_loss: 1.4338 - val_acc: 0.6054\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.3673 - acc: 0.8514 - val_loss: 1.4614 - val_acc: 0.6024\n",
      "Epoch 18/50\n",
      " - 12s - loss: 0.3390 - acc: 0.8474 - val_loss: 1.5400 - val_acc: 0.5994\n",
      "Epoch 19/50\n",
      " - 12s - loss: 0.3356 - acc: 0.8539 - val_loss: 1.4941 - val_acc: 0.6135\n",
      "Epoch 20/50\n",
      " - 12s - loss: 0.3359 - acc: 0.8544 - val_loss: 1.5060 - val_acc: 0.6084\n",
      "Epoch 00020: early stopping\n",
      "996/996 [==============================] - 1s 953us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8564 - acc: 0.3037 - val_loss: 1.6664 - val_acc: 0.3755\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.4986 - acc: 0.4513 - val_loss: 1.3597 - val_acc: 0.4980\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.1987 - acc: 0.5638 - val_loss: 1.1888 - val_acc: 0.5743\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.0084 - acc: 0.6411 - val_loss: 1.1277 - val_acc: 0.5964\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.8819 - acc: 0.6898 - val_loss: 1.1294 - val_acc: 0.5984\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.7635 - acc: 0.7249 - val_loss: 1.1079 - val_acc: 0.5984\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.6549 - acc: 0.7590 - val_loss: 1.1082 - val_acc: 0.6034\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.5827 - acc: 0.7866 - val_loss: 1.1348 - val_acc: 0.6114\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.5219 - acc: 0.8002 - val_loss: 1.1633 - val_acc: 0.6235\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.4939 - acc: 0.8077 - val_loss: 1.2348 - val_acc: 0.5994\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4740 - acc: 0.7997 - val_loss: 1.2530 - val_acc: 0.6175\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.4371 - acc: 0.8183 - val_loss: 1.2719 - val_acc: 0.5964\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4396 - acc: 0.8253 - val_loss: 1.2849 - val_acc: 0.6094\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.4130 - acc: 0.8323 - val_loss: 1.3172 - val_acc: 0.6185\n",
      "Epoch 00014: early stopping\n",
      "996/996 [==============================] - 1s 956us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8749 - acc: 0.2666 - val_loss: 1.6748 - val_acc: 0.3916\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.5265 - acc: 0.4528 - val_loss: 1.3864 - val_acc: 0.5141\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.2208 - acc: 0.5617 - val_loss: 1.2008 - val_acc: 0.5753\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.0324 - acc: 0.6481 - val_loss: 1.0932 - val_acc: 0.6235\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.8888 - acc: 0.6847 - val_loss: 1.0576 - val_acc: 0.6235\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.7641 - acc: 0.7239 - val_loss: 1.0959 - val_acc: 0.6225\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.6850 - acc: 0.7500 - val_loss: 1.0473 - val_acc: 0.6265\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.6138 - acc: 0.7756 - val_loss: 1.0486 - val_acc: 0.6426\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.5432 - acc: 0.7907 - val_loss: 1.1124 - val_acc: 0.6205\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.5207 - acc: 0.7831 - val_loss: 1.1290 - val_acc: 0.6235\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4684 - acc: 0.8143 - val_loss: 1.1802 - val_acc: 0.6145\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.4407 - acc: 0.8243 - val_loss: 1.2541 - val_acc: 0.6365\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4261 - acc: 0.8228 - val_loss: 1.2724 - val_acc: 0.6245\n",
      "Epoch 00013: early stopping\n",
      "996/996 [==============================] - 1s 958us/step\n",
      "Training with: 750\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8330 - acc: 0.3052 - val_loss: 1.7087 - val_acc: 0.3484\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.4433 - acc: 0.4639 - val_loss: 1.3975 - val_acc: 0.4849\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.1709 - acc: 0.5723 - val_loss: 1.3180 - val_acc: 0.5161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      " - 12s - loss: 0.9734 - acc: 0.6496 - val_loss: 1.2185 - val_acc: 0.5723\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.8170 - acc: 0.7083 - val_loss: 1.1570 - val_acc: 0.5783\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.7002 - acc: 0.7540 - val_loss: 1.2012 - val_acc: 0.5733\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.6097 - acc: 0.7846 - val_loss: 1.1913 - val_acc: 0.5823\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.5338 - acc: 0.8072 - val_loss: 1.2306 - val_acc: 0.6034\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.4915 - acc: 0.8102 - val_loss: 1.2580 - val_acc: 0.5954\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.4494 - acc: 0.8303 - val_loss: 1.3205 - val_acc: 0.6024\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4348 - acc: 0.8298 - val_loss: 1.3181 - val_acc: 0.6024\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.4049 - acc: 0.8368 - val_loss: 1.3618 - val_acc: 0.5944\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.4046 - acc: 0.8358 - val_loss: 1.3828 - val_acc: 0.6124\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.3798 - acc: 0.8409 - val_loss: 1.3793 - val_acc: 0.5964\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.3834 - acc: 0.8363 - val_loss: 1.4431 - val_acc: 0.6124\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.3666 - acc: 0.8494 - val_loss: 1.5750 - val_acc: 0.6064\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.3527 - acc: 0.8499 - val_loss: 1.4404 - val_acc: 0.6084\n",
      "Epoch 18/50\n",
      " - 12s - loss: 0.3379 - acc: 0.8559 - val_loss: 1.4827 - val_acc: 0.6084\n",
      "Epoch 00018: early stopping\n",
      "996/996 [==============================] - 1s 969us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8713 - acc: 0.2992 - val_loss: 1.6830 - val_acc: 0.4558\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.5035 - acc: 0.4563 - val_loss: 1.3494 - val_acc: 0.4940\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.1927 - acc: 0.5713 - val_loss: 1.1802 - val_acc: 0.5904\n",
      "Epoch 4/50\n",
      " - 12s - loss: 0.9695 - acc: 0.6621 - val_loss: 1.1341 - val_acc: 0.6024\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.8136 - acc: 0.7169 - val_loss: 1.0991 - val_acc: 0.6084\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.7018 - acc: 0.7485 - val_loss: 1.1320 - val_acc: 0.6225\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.6298 - acc: 0.7701 - val_loss: 1.1569 - val_acc: 0.6225\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.5862 - acc: 0.7751 - val_loss: 1.2113 - val_acc: 0.5833\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.5172 - acc: 0.7937 - val_loss: 1.2105 - val_acc: 0.5944\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.4783 - acc: 0.8002 - val_loss: 1.3001 - val_acc: 0.6155\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4452 - acc: 0.8153 - val_loss: 1.2714 - val_acc: 0.6215\n",
      "Epoch 00011: early stopping\n",
      "996/996 [==============================] - 1s 974us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 1.8467 - acc: 0.2887 - val_loss: 1.6398 - val_acc: 0.4036\n",
      "Epoch 2/50\n",
      " - 12s - loss: 1.4622 - acc: 0.4558 - val_loss: 1.3182 - val_acc: 0.5070\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.1883 - acc: 0.5648 - val_loss: 1.2339 - val_acc: 0.5562\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.0102 - acc: 0.6290 - val_loss: 1.1128 - val_acc: 0.6225\n",
      "Epoch 5/50\n",
      " - 12s - loss: 0.8585 - acc: 0.6867 - val_loss: 1.0324 - val_acc: 0.6355\n",
      "Epoch 6/50\n",
      " - 12s - loss: 0.7468 - acc: 0.7234 - val_loss: 1.0381 - val_acc: 0.6376\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.6402 - acc: 0.7666 - val_loss: 1.0547 - val_acc: 0.6245\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.5588 - acc: 0.7811 - val_loss: 1.1347 - val_acc: 0.6235\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.5305 - acc: 0.7917 - val_loss: 1.1253 - val_acc: 0.6165\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.4737 - acc: 0.8107 - val_loss: 1.1494 - val_acc: 0.6285\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.4487 - acc: 0.8243 - val_loss: 1.2179 - val_acc: 0.6295\n",
      "Epoch 00011: early stopping\n",
      "996/996 [==============================] - ETA:  - 1s 966us/step\n"
     ]
    }
   ],
   "source": [
    "# Test the impact of adding a hidden layer\n",
    "# Use the pre-trained weights from the previous run\n",
    "def two_layers(neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2000, input_dim=x_train.shape[1], activation='relu'))\n",
    "    if neurons > 0:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return (model)\n",
    "\n",
    "# Setting the input params\n",
    "neurons = [0, 250, 500, 750] # Number of neurons to test in the first hidden layer\n",
    "folds = 3 # Number of k-folds\n",
    "seed = 1986 # Random seed for reproducibility\n",
    "\n",
    "# Training the model\n",
    "gs2 = KerasClassifierGridSearchCV(neurons, folds, seed)\n",
    "gs2.fit(two_layers, x_train, y_train, epochs=50, batch_size=50, early_stopping_rounds=5, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6198125836680054,\n",
       " 250: 0.61713520749665329,\n",
       " 500: 0.61713520749665329,\n",
       " 750: 0.6198125836680054}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.get_avg_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61345381526104414, 0.61044176706827313, 0.63554216867469882]\n",
      "[0.60843373493975905, 0.62148594377510036, 0.62951807228915657]\n"
     ]
    }
   ],
   "source": [
    "print(gs2.get_results()[0])\n",
    "print(gs2.get_results()[750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model \n",
    "best_model = gs2.get_models()[750][2]\n",
    "\n",
    "# Save the model\n",
    "best_model.save('models/iteration2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/50\n",
      " - 66s - loss: 0.7252 - acc: 0.7390 - val_loss: 1.1159 - val_acc: 0.6186\n",
      "Epoch 2/50\n",
      " - 48s - loss: 0.5731 - acc: 0.7751 - val_loss: 1.0735 - val_acc: 0.6456\n",
      "Epoch 3/50\n",
      " - 25s - loss: 0.5208 - acc: 0.7928 - val_loss: 1.1424 - val_acc: 0.6336\n",
      "Epoch 4/50\n",
      " - 26s - loss: 0.4856 - acc: 0.7985 - val_loss: 1.1451 - val_acc: 0.6456\n",
      "Epoch 5/50\n",
      " - 51s - loss: 0.4449 - acc: 0.8156 - val_loss: 1.2051 - val_acc: 0.6607\n",
      "Epoch 6/50\n",
      " - 25s - loss: 0.4294 - acc: 0.8189 - val_loss: 1.3089 - val_acc: 0.6547\n",
      "Epoch 7/50\n",
      " - 25s - loss: 0.4149 - acc: 0.8233 - val_loss: 1.2015 - val_acc: 0.6547\n",
      "Epoch 8/50\n",
      " - 52s - loss: 0.4017 - acc: 0.8266 - val_loss: 1.2672 - val_acc: 0.6667\n",
      "Epoch 9/50\n",
      " - 26s - loss: 0.3918 - acc: 0.8303 - val_loss: 1.3022 - val_acc: 0.6547\n",
      "Epoch 10/50\n",
      " - 26s - loss: 0.3819 - acc: 0.8333 - val_loss: 1.2936 - val_acc: 0.6396\n",
      "Epoch 11/50\n",
      " - 26s - loss: 0.3794 - acc: 0.8317 - val_loss: 1.2681 - val_acc: 0.6547\n",
      "Epoch 12/50\n",
      " - 56s - loss: 0.3677 - acc: 0.8377 - val_loss: 1.3473 - val_acc: 0.6697\n",
      "Epoch 13/50\n",
      " - 25s - loss: 0.3740 - acc: 0.8370 - val_loss: 1.4025 - val_acc: 0.6697\n",
      "Epoch 14/50\n",
      " - 25s - loss: 0.3596 - acc: 0.8353 - val_loss: 1.3856 - val_acc: 0.6607\n",
      "Epoch 15/50\n",
      " - 25s - loss: 0.3560 - acc: 0.8360 - val_loss: 1.3664 - val_acc: 0.6577\n",
      "Epoch 16/50\n",
      " - 25s - loss: 0.3328 - acc: 0.8477 - val_loss: 1.4850 - val_acc: 0.6547\n",
      "Epoch 17/50\n",
      " - 26s - loss: 0.3382 - acc: 0.8464 - val_loss: 1.4075 - val_acc: 0.6667\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa59f739d68>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=5, verbose=True)\n",
    "\n",
    "# Checkpoint - used to get the best weights during the model training process\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_weights.h5', monitor='val_acc', save_best_only=True)\n",
    "\n",
    "# Fit the pre-trained best model on the entire dataset\n",
    "best_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping, checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEmCAYAAAD1FIKpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa59f71eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best iteration\n",
    "best_model = load_model('models/best_weights.h5')\n",
    "\n",
    "# Make the predictions\n",
    "preds = best_model.predict_classes(x_test)\n",
    "preds = [i + 1 for i in preds]\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test_true, preds, labels=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[1,2,3,4,5,6,7,8,9], title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66967\n",
      "333/333 [==============================] - 1s 2ms/step\n",
      "[1.3472904475243599, 0.6696696703856414]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "acc = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print('Accuracy: %0.5f' % acc)\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model in case we want to reference it later\n",
    "best_model.save('models/iteration2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model actually performs better with only a single hidden layer. Next, let's test dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: 0\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9371 - acc: 0.2987 - val_loss: 1.8356 - val_acc: 0.2540\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7001 - acc: 0.3474 - val_loss: 1.7119 - val_acc: 0.2972\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5543 - acc: 0.4357 - val_loss: 1.5564 - val_acc: 0.4518\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4026 - acc: 0.5266 - val_loss: 1.4312 - val_acc: 0.4970\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2709 - acc: 0.5713 - val_loss: 1.3421 - val_acc: 0.5201\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1636 - acc: 0.6124 - val_loss: 1.2806 - val_acc: 0.5502\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0778 - acc: 0.6511 - val_loss: 1.2387 - val_acc: 0.5572\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0073 - acc: 0.6647 - val_loss: 1.2013 - val_acc: 0.5853\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9457 - acc: 0.6893 - val_loss: 1.1722 - val_acc: 0.5823\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.8894 - acc: 0.7134 - val_loss: 1.1638 - val_acc: 0.5813\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8386 - acc: 0.7229 - val_loss: 1.1450 - val_acc: 0.5813\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.7909 - acc: 0.7435 - val_loss: 1.1423 - val_acc: 0.5904\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7546 - acc: 0.7475 - val_loss: 1.1205 - val_acc: 0.5914\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7066 - acc: 0.7641 - val_loss: 1.0982 - val_acc: 0.5873\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.6744 - acc: 0.7756 - val_loss: 1.0968 - val_acc: 0.5904\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6420 - acc: 0.7816 - val_loss: 1.0906 - val_acc: 0.5914\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6130 - acc: 0.7892 - val_loss: 1.0823 - val_acc: 0.5914\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.5822 - acc: 0.8057 - val_loss: 1.1026 - val_acc: 0.5984\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.5595 - acc: 0.8072 - val_loss: 1.0989 - val_acc: 0.6044\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.5412 - acc: 0.8092 - val_loss: 1.0968 - val_acc: 0.6024\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.5184 - acc: 0.8208 - val_loss: 1.0928 - val_acc: 0.6024\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.4953 - acc: 0.8288 - val_loss: 1.1009 - val_acc: 0.5974\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.4814 - acc: 0.8288 - val_loss: 1.0946 - val_acc: 0.6044\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.4659 - acc: 0.8333 - val_loss: 1.1132 - val_acc: 0.5974\n",
      "Epoch 00024: early stopping\n",
      "996/996 [==============================] - 1s 818us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9406 - acc: 0.2806 - val_loss: 1.7584 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7029 - acc: 0.3705 - val_loss: 1.6121 - val_acc: 0.4237\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5276 - acc: 0.4659 - val_loss: 1.4567 - val_acc: 0.5161\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.3702 - acc: 0.5336 - val_loss: 1.3446 - val_acc: 0.5442\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2454 - acc: 0.5884 - val_loss: 1.2617 - val_acc: 0.5763\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1433 - acc: 0.6290 - val_loss: 1.2117 - val_acc: 0.5924\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0613 - acc: 0.6571 - val_loss: 1.1685 - val_acc: 0.6054\n",
      "Epoch 8/50\n",
      " - 9s - loss: 0.9927 - acc: 0.6837 - val_loss: 1.1344 - val_acc: 0.6114\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9312 - acc: 0.6888 - val_loss: 1.1254 - val_acc: 0.6145\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.8735 - acc: 0.7068 - val_loss: 1.0989 - val_acc: 0.6155\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8228 - acc: 0.7274 - val_loss: 1.0907 - val_acc: 0.6104\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.7770 - acc: 0.7339 - val_loss: 1.0879 - val_acc: 0.6145\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7368 - acc: 0.7530 - val_loss: 1.0662 - val_acc: 0.6245\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7033 - acc: 0.7565 - val_loss: 1.0732 - val_acc: 0.6175\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.6632 - acc: 0.7701 - val_loss: 1.0632 - val_acc: 0.6195\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6365 - acc: 0.7711 - val_loss: 1.0577 - val_acc: 0.6165\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6045 - acc: 0.7821 - val_loss: 1.0603 - val_acc: 0.6205\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.5794 - acc: 0.7887 - val_loss: 1.0657 - val_acc: 0.6084\n",
      "Epoch 00018: early stopping\n",
      "996/996 [==============================] - 1s 822us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9467 - acc: 0.2751 - val_loss: 1.7411 - val_acc: 0.3253\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7045 - acc: 0.3685 - val_loss: 1.6016 - val_acc: 0.4598\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5300 - acc: 0.4734 - val_loss: 1.4451 - val_acc: 0.4769\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.3706 - acc: 0.5311 - val_loss: 1.3300 - val_acc: 0.5452\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2505 - acc: 0.5894 - val_loss: 1.2505 - val_acc: 0.5853\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1564 - acc: 0.6220 - val_loss: 1.2021 - val_acc: 0.5944\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0803 - acc: 0.6436 - val_loss: 1.1467 - val_acc: 0.6084\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0105 - acc: 0.6596 - val_loss: 1.1107 - val_acc: 0.6215\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9525 - acc: 0.6767 - val_loss: 1.0728 - val_acc: 0.6255\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.8947 - acc: 0.6948 - val_loss: 1.0537 - val_acc: 0.6325\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8430 - acc: 0.7028 - val_loss: 1.0315 - val_acc: 0.6305\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.7961 - acc: 0.7254 - val_loss: 1.0132 - val_acc: 0.6355\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7558 - acc: 0.7374 - val_loss: 1.0009 - val_acc: 0.6355\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7158 - acc: 0.7515 - val_loss: 0.9925 - val_acc: 0.6406\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.6791 - acc: 0.7565 - val_loss: 0.9939 - val_acc: 0.6275\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6481 - acc: 0.7716 - val_loss: 0.9866 - val_acc: 0.6315\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6183 - acc: 0.7821 - val_loss: 0.9945 - val_acc: 0.6365\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.5929 - acc: 0.7972 - val_loss: 0.9835 - val_acc: 0.6355\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.5643 - acc: 0.7977 - val_loss: 0.9795 - val_acc: 0.6476\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.5440 - acc: 0.8042 - val_loss: 0.9820 - val_acc: 0.6496\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.5248 - acc: 0.8072 - val_loss: 0.9998 - val_acc: 0.6245\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.5064 - acc: 0.8193 - val_loss: 1.0021 - val_acc: 0.6426\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.4885 - acc: 0.8238 - val_loss: 1.0064 - val_acc: 0.6275\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.4765 - acc: 0.8203 - val_loss: 1.0018 - val_acc: 0.6325\n",
      "Epoch 25/50\n",
      " - 9s - loss: 0.4614 - acc: 0.8338 - val_loss: 1.0149 - val_acc: 0.6516\n",
      "Epoch 26/50\n",
      " - 9s - loss: 0.4480 - acc: 0.8373 - val_loss: 1.0287 - val_acc: 0.6426\n",
      "Epoch 27/50\n",
      " - 9s - loss: 0.4357 - acc: 0.8368 - val_loss: 1.0390 - val_acc: 0.6345\n",
      "Epoch 28/50\n",
      " - 9s - loss: 0.4310 - acc: 0.8434 - val_loss: 1.0276 - val_acc: 0.6456\n",
      "Epoch 29/50\n",
      " - 9s - loss: 0.4159 - acc: 0.8394 - val_loss: 1.0524 - val_acc: 0.6456\n",
      "Epoch 30/50\n",
      " - 9s - loss: 0.4095 - acc: 0.8394 - val_loss: 1.0574 - val_acc: 0.6426\n",
      "Epoch 00030: early stopping\n",
      "996/996 [==============================] - 1s 809us/step\n",
      "Training with: 0.25\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 10s - loss: 1.9265 - acc: 0.2982 - val_loss: 1.8313 - val_acc: 0.2540\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.6940 - acc: 0.3479 - val_loss: 1.6895 - val_acc: 0.3705\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5410 - acc: 0.4508 - val_loss: 1.5308 - val_acc: 0.4568\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.3898 - acc: 0.5196 - val_loss: 1.4183 - val_acc: 0.4890\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2653 - acc: 0.5658 - val_loss: 1.3341 - val_acc: 0.5211\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.1719 - acc: 0.6079 - val_loss: 1.2717 - val_acc: 0.5562\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0934 - acc: 0.6386 - val_loss: 1.2386 - val_acc: 0.5532\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0261 - acc: 0.6627 - val_loss: 1.2192 - val_acc: 0.5582\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.9645 - acc: 0.6757 - val_loss: 1.1871 - val_acc: 0.5733\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.9108 - acc: 0.7038 - val_loss: 1.1593 - val_acc: 0.5753\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.8611 - acc: 0.7199 - val_loss: 1.1533 - val_acc: 0.5723\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.8169 - acc: 0.7339 - val_loss: 1.1250 - val_acc: 0.5753\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.7803 - acc: 0.7490 - val_loss: 1.1103 - val_acc: 0.5853\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7444 - acc: 0.7455 - val_loss: 1.1173 - val_acc: 0.5863\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.7061 - acc: 0.7605 - val_loss: 1.1164 - val_acc: 0.5924\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.6778 - acc: 0.7746 - val_loss: 1.1001 - val_acc: 0.5853\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6449 - acc: 0.7826 - val_loss: 1.0855 - val_acc: 0.5984\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.6109 - acc: 0.7937 - val_loss: 1.0988 - val_acc: 0.6004\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.5910 - acc: 0.8007 - val_loss: 1.0833 - val_acc: 0.5994\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.5782 - acc: 0.7982 - val_loss: 1.1210 - val_acc: 0.5934\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.5539 - acc: 0.8077 - val_loss: 1.1058 - val_acc: 0.5944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      " - 9s - loss: 0.5351 - acc: 0.8092 - val_loss: 1.0896 - val_acc: 0.6004\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.5203 - acc: 0.8208 - val_loss: 1.0962 - val_acc: 0.6034\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.4948 - acc: 0.8238 - val_loss: 1.1108 - val_acc: 0.5934\n",
      "Epoch 25/50\n",
      " - 9s - loss: 0.4866 - acc: 0.8258 - val_loss: 1.1077 - val_acc: 0.6104\n",
      "Epoch 26/50\n",
      " - 9s - loss: 0.4697 - acc: 0.8338 - val_loss: 1.1263 - val_acc: 0.6034\n",
      "Epoch 27/50\n",
      " - 9s - loss: 0.4580 - acc: 0.8313 - val_loss: 1.1541 - val_acc: 0.6074\n",
      "Epoch 28/50\n",
      " - 9s - loss: 0.4452 - acc: 0.8363 - val_loss: 1.1251 - val_acc: 0.6054\n",
      "Epoch 29/50\n",
      " - 9s - loss: 0.4350 - acc: 0.8409 - val_loss: 1.1400 - val_acc: 0.6135\n",
      "Epoch 30/50\n",
      " - 9s - loss: 0.4295 - acc: 0.8414 - val_loss: 1.1517 - val_acc: 0.6124\n",
      "Epoch 31/50\n",
      " - 9s - loss: 0.4131 - acc: 0.8484 - val_loss: 1.1384 - val_acc: 0.6004\n",
      "Epoch 32/50\n",
      " - 9s - loss: 0.4190 - acc: 0.8399 - val_loss: 1.1672 - val_acc: 0.6024\n",
      "Epoch 33/50\n",
      " - 9s - loss: 0.4038 - acc: 0.8494 - val_loss: 1.1548 - val_acc: 0.6044\n",
      "Epoch 34/50\n",
      " - 9s - loss: 0.3959 - acc: 0.8544 - val_loss: 1.1675 - val_acc: 0.6145\n",
      "Epoch 35/50\n",
      " - 9s - loss: 0.3943 - acc: 0.8469 - val_loss: 1.1579 - val_acc: 0.6014\n",
      "Epoch 36/50\n",
      " - 9s - loss: 0.3799 - acc: 0.8574 - val_loss: 1.1934 - val_acc: 0.6074\n",
      "Epoch 37/50\n",
      " - 9s - loss: 0.3844 - acc: 0.8519 - val_loss: 1.2196 - val_acc: 0.6084\n",
      "Epoch 38/50\n",
      " - 9s - loss: 0.3763 - acc: 0.8504 - val_loss: 1.1853 - val_acc: 0.5984\n",
      "Epoch 39/50\n",
      " - 9s - loss: 0.3727 - acc: 0.8444 - val_loss: 1.2086 - val_acc: 0.6074\n",
      "Epoch 00039: early stopping\n",
      "996/996 [==============================] - 1s 826us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9519 - acc: 0.2751 - val_loss: 1.7726 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7278 - acc: 0.3399 - val_loss: 1.6505 - val_acc: 0.3855\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5811 - acc: 0.4372 - val_loss: 1.5092 - val_acc: 0.4970\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4310 - acc: 0.5090 - val_loss: 1.3889 - val_acc: 0.5090\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.3080 - acc: 0.5673 - val_loss: 1.3042 - val_acc: 0.5422\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.2187 - acc: 0.5909 - val_loss: 1.2503 - val_acc: 0.5472\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.1320 - acc: 0.6280 - val_loss: 1.1970 - val_acc: 0.5813\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0625 - acc: 0.6431 - val_loss: 1.1662 - val_acc: 0.5944\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.0040 - acc: 0.6732 - val_loss: 1.1350 - val_acc: 0.6124\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.9402 - acc: 0.6923 - val_loss: 1.1178 - val_acc: 0.6114\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.9014 - acc: 0.6993 - val_loss: 1.0989 - val_acc: 0.6185\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.8508 - acc: 0.7088 - val_loss: 1.0894 - val_acc: 0.6124\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.8164 - acc: 0.7164 - val_loss: 1.0822 - val_acc: 0.6215\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7765 - acc: 0.7369 - val_loss: 1.0714 - val_acc: 0.6205\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.7376 - acc: 0.7495 - val_loss: 1.0724 - val_acc: 0.6145\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.7030 - acc: 0.7550 - val_loss: 1.0610 - val_acc: 0.6185\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6692 - acc: 0.7656 - val_loss: 1.0624 - val_acc: 0.6155\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.6536 - acc: 0.7691 - val_loss: 1.0576 - val_acc: 0.6185\n",
      "Epoch 00018: early stopping\n",
      "996/996 [==============================] - 1s 827us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9561 - acc: 0.2711 - val_loss: 1.7512 - val_acc: 0.3082\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7219 - acc: 0.3414 - val_loss: 1.6209 - val_acc: 0.4498\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5601 - acc: 0.4644 - val_loss: 1.4742 - val_acc: 0.4970\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4107 - acc: 0.5221 - val_loss: 1.3609 - val_acc: 0.5452\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.2940 - acc: 0.5858 - val_loss: 1.2844 - val_acc: 0.5793\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.2008 - acc: 0.6124 - val_loss: 1.2190 - val_acc: 0.5803\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.1227 - acc: 0.6290 - val_loss: 1.1687 - val_acc: 0.6145\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0571 - acc: 0.6526 - val_loss: 1.1264 - val_acc: 0.6034\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.0035 - acc: 0.6647 - val_loss: 1.0972 - val_acc: 0.6305\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.9502 - acc: 0.6837 - val_loss: 1.0764 - val_acc: 0.6285\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.9033 - acc: 0.6918 - val_loss: 1.0696 - val_acc: 0.6195\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.8585 - acc: 0.7063 - val_loss: 1.0273 - val_acc: 0.6376\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.8110 - acc: 0.7204 - val_loss: 1.0133 - val_acc: 0.6376\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.7704 - acc: 0.7334 - val_loss: 1.0067 - val_acc: 0.6426\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.7357 - acc: 0.7420 - val_loss: 1.0085 - val_acc: 0.6376\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.7114 - acc: 0.7555 - val_loss: 0.9890 - val_acc: 0.6345\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.6758 - acc: 0.7656 - val_loss: 1.0037 - val_acc: 0.6255\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.6482 - acc: 0.7776 - val_loss: 0.9909 - val_acc: 0.6386\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.6225 - acc: 0.7771 - val_loss: 0.9958 - val_acc: 0.6345\n",
      "Epoch 00019: early stopping\n",
      "996/996 [==============================] - 1s 822us/step\n",
      "Training with: 0.5\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9323 - acc: 0.3022 - val_loss: 1.8335 - val_acc: 0.2530\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7052 - acc: 0.3489 - val_loss: 1.7049 - val_acc: 0.3755\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5692 - acc: 0.4312 - val_loss: 1.5676 - val_acc: 0.4317\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4373 - acc: 0.4945 - val_loss: 1.4583 - val_acc: 0.4568\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.3288 - acc: 0.5447 - val_loss: 1.3835 - val_acc: 0.5050\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.2393 - acc: 0.5723 - val_loss: 1.3357 - val_acc: 0.5090\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.1683 - acc: 0.6150 - val_loss: 1.2780 - val_acc: 0.5472\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.0998 - acc: 0.6315 - val_loss: 1.2374 - val_acc: 0.5532\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.0452 - acc: 0.6586 - val_loss: 1.2103 - val_acc: 0.5592\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.9922 - acc: 0.6727 - val_loss: 1.2006 - val_acc: 0.5673\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.9485 - acc: 0.6807 - val_loss: 1.1695 - val_acc: 0.5813\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.9092 - acc: 0.6993 - val_loss: 1.1529 - val_acc: 0.5813\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.8743 - acc: 0.7129 - val_loss: 1.1410 - val_acc: 0.5974\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.8371 - acc: 0.7214 - val_loss: 1.1346 - val_acc: 0.5934\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.7914 - acc: 0.7380 - val_loss: 1.1166 - val_acc: 0.5763\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.7774 - acc: 0.7395 - val_loss: 1.1231 - val_acc: 0.5924\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.7395 - acc: 0.7520 - val_loss: 1.1346 - val_acc: 0.5803\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.7199 - acc: 0.7500 - val_loss: 1.0927 - val_acc: 0.6014\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.6901 - acc: 0.7615 - val_loss: 1.1194 - val_acc: 0.5873\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.6724 - acc: 0.7636 - val_loss: 1.0865 - val_acc: 0.6064\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.6480 - acc: 0.7746 - val_loss: 1.0874 - val_acc: 0.5974\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.6244 - acc: 0.7826 - val_loss: 1.0970 - val_acc: 0.6044\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.6039 - acc: 0.7811 - val_loss: 1.1089 - val_acc: 0.6044\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.5920 - acc: 0.7947 - val_loss: 1.1225 - val_acc: 0.5863\n",
      "Epoch 25/50\n",
      " - 9s - loss: 0.5779 - acc: 0.7987 - val_loss: 1.1050 - val_acc: 0.5984\n",
      "Epoch 00025: early stopping\n",
      "996/996 [==============================] - 1s 820us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9485 - acc: 0.2741 - val_loss: 1.7730 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7295 - acc: 0.3454 - val_loss: 1.6555 - val_acc: 0.3745\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.5887 - acc: 0.4388 - val_loss: 1.5178 - val_acc: 0.4769\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4459 - acc: 0.5015 - val_loss: 1.4046 - val_acc: 0.5281\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.3313 - acc: 0.5537 - val_loss: 1.3175 - val_acc: 0.5442\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.2424 - acc: 0.5884 - val_loss: 1.2590 - val_acc: 0.5703\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.1714 - acc: 0.6170 - val_loss: 1.2160 - val_acc: 0.5653\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.1085 - acc: 0.6365 - val_loss: 1.1756 - val_acc: 0.5924\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.0543 - acc: 0.6491 - val_loss: 1.1607 - val_acc: 0.6004\n",
      "Epoch 10/50\n",
      " - 9s - loss: 0.9997 - acc: 0.6611 - val_loss: 1.1483 - val_acc: 0.5944\n",
      "Epoch 11/50\n",
      " - 9s - loss: 0.9599 - acc: 0.6867 - val_loss: 1.1294 - val_acc: 0.6094\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.9160 - acc: 0.6918 - val_loss: 1.1113 - val_acc: 0.6024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      " - 9s - loss: 0.8788 - acc: 0.7058 - val_loss: 1.0903 - val_acc: 0.6225\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.8458 - acc: 0.7129 - val_loss: 1.0840 - val_acc: 0.6205\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.7978 - acc: 0.7314 - val_loss: 1.0730 - val_acc: 0.6195\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.7824 - acc: 0.7254 - val_loss: 1.0692 - val_acc: 0.6245\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.7531 - acc: 0.7435 - val_loss: 1.0632 - val_acc: 0.6225\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.7251 - acc: 0.7460 - val_loss: 1.0710 - val_acc: 0.6135\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.7064 - acc: 0.7565 - val_loss: 1.0608 - val_acc: 0.6114\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.6817 - acc: 0.7595 - val_loss: 1.0588 - val_acc: 0.6205\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.6718 - acc: 0.7620 - val_loss: 1.0561 - val_acc: 0.6195\n",
      "Epoch 00021: early stopping\n",
      "996/996 [==============================] - 1s 823us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9612 - acc: 0.2741 - val_loss: 1.7751 - val_acc: 0.3193\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7488 - acc: 0.3223 - val_loss: 1.6633 - val_acc: 0.4227\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.6174 - acc: 0.4287 - val_loss: 1.5276 - val_acc: 0.4649\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.4829 - acc: 0.4824 - val_loss: 1.4141 - val_acc: 0.5080\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.3737 - acc: 0.5311 - val_loss: 1.3351 - val_acc: 0.5492\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.2934 - acc: 0.5683 - val_loss: 1.2733 - val_acc: 0.5673\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.2175 - acc: 0.5969 - val_loss: 1.2169 - val_acc: 0.5863\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.1561 - acc: 0.6180 - val_loss: 1.1726 - val_acc: 0.5964\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.1058 - acc: 0.6190 - val_loss: 1.1499 - val_acc: 0.6074\n",
      "Epoch 10/50\n",
      " - 9s - loss: 1.0469 - acc: 0.6536 - val_loss: 1.1129 - val_acc: 0.6145\n",
      "Epoch 11/50\n",
      " - 9s - loss: 1.0055 - acc: 0.6551 - val_loss: 1.0922 - val_acc: 0.6245\n",
      "Epoch 12/50\n",
      " - 9s - loss: 0.9654 - acc: 0.6727 - val_loss: 1.0841 - val_acc: 0.6155\n",
      "Epoch 13/50\n",
      " - 9s - loss: 0.9329 - acc: 0.6742 - val_loss: 1.0564 - val_acc: 0.6275\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.8965 - acc: 0.6867 - val_loss: 1.0343 - val_acc: 0.6315\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.8470 - acc: 0.7048 - val_loss: 1.0234 - val_acc: 0.6275\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.8231 - acc: 0.7139 - val_loss: 1.0128 - val_acc: 0.6305\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.7957 - acc: 0.7264 - val_loss: 1.0067 - val_acc: 0.6345\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.7594 - acc: 0.7364 - val_loss: 0.9899 - val_acc: 0.6376\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.7393 - acc: 0.7405 - val_loss: 0.9854 - val_acc: 0.6345\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.7206 - acc: 0.7420 - val_loss: 0.9806 - val_acc: 0.6446\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.6910 - acc: 0.7570 - val_loss: 0.9972 - val_acc: 0.6325\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.6729 - acc: 0.7615 - val_loss: 0.9960 - val_acc: 0.6325\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.6564 - acc: 0.7646 - val_loss: 0.9852 - val_acc: 0.6305\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.6271 - acc: 0.7796 - val_loss: 0.9852 - val_acc: 0.6345\n",
      "Epoch 25/50\n",
      " - 9s - loss: 0.6150 - acc: 0.7816 - val_loss: 0.9809 - val_acc: 0.6365\n",
      "Epoch 00025: early stopping\n",
      "996/996 [==============================] - 1s 827us/step\n",
      "Training with: 0.75\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9377 - acc: 0.2992 - val_loss: 1.8487 - val_acc: 0.2530\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7353 - acc: 0.3303 - val_loss: 1.7422 - val_acc: 0.3082\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.6296 - acc: 0.4031 - val_loss: 1.6317 - val_acc: 0.3815\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.5091 - acc: 0.4649 - val_loss: 1.5240 - val_acc: 0.4428\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.4027 - acc: 0.5201 - val_loss: 1.4267 - val_acc: 0.4839\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.3383 - acc: 0.5306 - val_loss: 1.3681 - val_acc: 0.5291\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.2599 - acc: 0.5773 - val_loss: 1.3395 - val_acc: 0.5110\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.2145 - acc: 0.5879 - val_loss: 1.2960 - val_acc: 0.5341\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.1641 - acc: 0.6024 - val_loss: 1.2556 - val_acc: 0.5562\n",
      "Epoch 10/50\n",
      " - 9s - loss: 1.1162 - acc: 0.6260 - val_loss: 1.2500 - val_acc: 0.5653\n",
      "Epoch 11/50\n",
      " - 9s - loss: 1.0791 - acc: 0.6355 - val_loss: 1.2220 - val_acc: 0.5612\n",
      "Epoch 12/50\n",
      " - 9s - loss: 1.0430 - acc: 0.6451 - val_loss: 1.2109 - val_acc: 0.5673\n",
      "Epoch 13/50\n",
      " - 9s - loss: 1.0149 - acc: 0.6637 - val_loss: 1.1848 - val_acc: 0.5723\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.9710 - acc: 0.6717 - val_loss: 1.1685 - val_acc: 0.5823\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.9431 - acc: 0.6817 - val_loss: 1.1652 - val_acc: 0.5853\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.9215 - acc: 0.6928 - val_loss: 1.1471 - val_acc: 0.5813\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.9004 - acc: 0.6872 - val_loss: 1.1556 - val_acc: 0.5743\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.8730 - acc: 0.6998 - val_loss: 1.1452 - val_acc: 0.5904\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.8566 - acc: 0.7033 - val_loss: 1.1298 - val_acc: 0.5894\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.8313 - acc: 0.7199 - val_loss: 1.1273 - val_acc: 0.5884\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.8041 - acc: 0.7269 - val_loss: 1.1151 - val_acc: 0.5873\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.7967 - acc: 0.7239 - val_loss: 1.1118 - val_acc: 0.5823\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.7684 - acc: 0.7319 - val_loss: 1.1086 - val_acc: 0.5793\n",
      "Epoch 00023: early stopping\n",
      "996/996 [==============================] - 1s 825us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9602 - acc: 0.2856 - val_loss: 1.7809 - val_acc: 0.3012\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7552 - acc: 0.3198 - val_loss: 1.6883 - val_acc: 0.3645\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.6383 - acc: 0.4051 - val_loss: 1.5694 - val_acc: 0.4367\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.5240 - acc: 0.4608 - val_loss: 1.4650 - val_acc: 0.5070\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.4267 - acc: 0.5115 - val_loss: 1.3839 - val_acc: 0.5472\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.3457 - acc: 0.5286 - val_loss: 1.3165 - val_acc: 0.5261\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.2860 - acc: 0.5622 - val_loss: 1.2755 - val_acc: 0.5402\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.2201 - acc: 0.5954 - val_loss: 1.2367 - val_acc: 0.5572\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.1763 - acc: 0.6014 - val_loss: 1.2074 - val_acc: 0.5733\n",
      "Epoch 10/50\n",
      " - 9s - loss: 1.1391 - acc: 0.6200 - val_loss: 1.1859 - val_acc: 0.5944\n",
      "Epoch 11/50\n",
      " - 9s - loss: 1.0999 - acc: 0.6386 - val_loss: 1.1676 - val_acc: 0.5934\n",
      "Epoch 12/50\n",
      " - 9s - loss: 1.0606 - acc: 0.6426 - val_loss: 1.1478 - val_acc: 0.5984\n",
      "Epoch 13/50\n",
      " - 9s - loss: 1.0346 - acc: 0.6521 - val_loss: 1.1306 - val_acc: 0.6074\n",
      "Epoch 14/50\n",
      " - 9s - loss: 0.9920 - acc: 0.6621 - val_loss: 1.1195 - val_acc: 0.6034\n",
      "Epoch 15/50\n",
      " - 9s - loss: 0.9558 - acc: 0.6606 - val_loss: 1.1152 - val_acc: 0.6084\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.9264 - acc: 0.6867 - val_loss: 1.0990 - val_acc: 0.6185\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.9029 - acc: 0.6832 - val_loss: 1.0953 - val_acc: 0.6064\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.8836 - acc: 0.6943 - val_loss: 1.0833 - val_acc: 0.6185\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.8779 - acc: 0.6923 - val_loss: 1.0956 - val_acc: 0.6104\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.8327 - acc: 0.7043 - val_loss: 1.0811 - val_acc: 0.6114\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.8143 - acc: 0.7113 - val_loss: 1.0714 - val_acc: 0.6165\n",
      "Epoch 00021: early stopping\n",
      "996/996 [==============================] - 1s 814us/step\n",
      "Train on 1992 samples, validate on 996 samples\n",
      "Epoch 1/50\n",
      " - 11s - loss: 1.9704 - acc: 0.2716 - val_loss: 1.7744 - val_acc: 0.3082\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.7676 - acc: 0.3168 - val_loss: 1.6883 - val_acc: 0.4177\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.6556 - acc: 0.4066 - val_loss: 1.5641 - val_acc: 0.4498\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.5369 - acc: 0.4538 - val_loss: 1.4656 - val_acc: 0.5251\n",
      "Epoch 5/50\n",
      " - 9s - loss: 1.4320 - acc: 0.5146 - val_loss: 1.3740 - val_acc: 0.5422\n",
      "Epoch 6/50\n",
      " - 9s - loss: 1.3518 - acc: 0.5462 - val_loss: 1.3140 - val_acc: 0.5432\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.3095 - acc: 0.5517 - val_loss: 1.2696 - val_acc: 0.5904\n",
      "Epoch 8/50\n",
      " - 9s - loss: 1.2526 - acc: 0.5778 - val_loss: 1.2310 - val_acc: 0.5894\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.1936 - acc: 0.6024 - val_loss: 1.1937 - val_acc: 0.6084\n",
      "Epoch 10/50\n",
      " - 9s - loss: 1.1605 - acc: 0.5964 - val_loss: 1.1701 - val_acc: 0.5773\n",
      "Epoch 11/50\n",
      " - 9s - loss: 1.1228 - acc: 0.6195 - val_loss: 1.1541 - val_acc: 0.5934\n",
      "Epoch 12/50\n",
      " - 9s - loss: 1.0897 - acc: 0.6345 - val_loss: 1.1244 - val_acc: 0.6024\n",
      "Epoch 13/50\n",
      " - 9s - loss: 1.0523 - acc: 0.6411 - val_loss: 1.0905 - val_acc: 0.6235\n",
      "Epoch 14/50\n",
      " - 9s - loss: 1.0150 - acc: 0.6511 - val_loss: 1.0786 - val_acc: 0.6215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      " - 9s - loss: 0.9954 - acc: 0.6381 - val_loss: 1.0666 - val_acc: 0.6285\n",
      "Epoch 16/50\n",
      " - 9s - loss: 0.9832 - acc: 0.6672 - val_loss: 1.0540 - val_acc: 0.6325\n",
      "Epoch 17/50\n",
      " - 9s - loss: 0.9552 - acc: 0.6621 - val_loss: 1.0453 - val_acc: 0.6305\n",
      "Epoch 18/50\n",
      " - 9s - loss: 0.9148 - acc: 0.6842 - val_loss: 1.0264 - val_acc: 0.6285\n",
      "Epoch 19/50\n",
      " - 9s - loss: 0.8842 - acc: 0.6842 - val_loss: 1.0426 - val_acc: 0.6135\n",
      "Epoch 20/50\n",
      " - 9s - loss: 0.8522 - acc: 0.7028 - val_loss: 1.0142 - val_acc: 0.6446\n",
      "Epoch 21/50\n",
      " - 9s - loss: 0.8458 - acc: 0.6953 - val_loss: 1.0063 - val_acc: 0.6396\n",
      "Epoch 22/50\n",
      " - 9s - loss: 0.8397 - acc: 0.6988 - val_loss: 1.0056 - val_acc: 0.6315\n",
      "Epoch 23/50\n",
      " - 9s - loss: 0.8039 - acc: 0.7274 - val_loss: 1.0017 - val_acc: 0.6456\n",
      "Epoch 24/50\n",
      " - 9s - loss: 0.7889 - acc: 0.7154 - val_loss: 0.9934 - val_acc: 0.6406\n",
      "Epoch 25/50\n",
      " - 9s - loss: 0.7719 - acc: 0.7259 - val_loss: 0.9874 - val_acc: 0.6365\n",
      "Epoch 26/50\n",
      " - 9s - loss: 0.7626 - acc: 0.7415 - val_loss: 0.9936 - val_acc: 0.6345\n",
      "Epoch 27/50\n",
      " - 9s - loss: 0.7390 - acc: 0.7339 - val_loss: 0.9905 - val_acc: 0.6345\n",
      "Epoch 28/50\n",
      " - 9s - loss: 0.7213 - acc: 0.7475 - val_loss: 0.9821 - val_acc: 0.6386\n",
      "Epoch 00028: early stopping\n",
      "996/996 [==============================] - 1s 832us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_model('models/iteration1.h5')\n",
    "\n",
    "# Test the impact of adding a dropout layer\n",
    "# Use the pre-trained weights from the previous run\n",
    "def dropout(rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=rate, input_shape=(x_train.shape[1],)))\n",
    "    model.add(Dense(1500, activation='relu'))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return (model)\n",
    "\n",
    "# Setting the input params\n",
    "rate = [0, 0.25, 0.5, 0.75] # Dropout rates\n",
    "folds = 3 # Number of k-folds\n",
    "seed = 1986 # Random seed for reproducibility\n",
    "\n",
    "# Training the model\n",
    "gs3 = KerasClassifierGridSearchCV(rate, folds, seed)\n",
    "gs3.fit(dropout, x_train, y_train, epochs=50, batch_size=50, early_stopping_rounds=5, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.61613119143239625, 0.25: 0.62014725568942441, 0.5: 0.61813922356091033, 0.75: 0.61144578313253006}\n"
     ]
    }
   ],
   "source": [
    "print(gs3.get_avg_results())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60742971887550201, 0.61847389558232935, 0.63453815261044177]\n"
     ]
    }
   ],
   "source": [
    "print(gs3.get_results()[0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model \n",
    "best_model = gs3.get_models()[0.25][2]\n",
    "\n",
    "# Save the model\n",
    "best_model.save('models/iteration3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2988 samples, validate on 333 samples\n",
      "Epoch 1/50\n",
      " - 22s - loss: 0.7480 - acc: 0.7296 - val_loss: 1.0098 - val_acc: 0.6216\n",
      "Epoch 2/50\n",
      " - 22s - loss: 0.6938 - acc: 0.7426 - val_loss: 1.0223 - val_acc: 0.6216\n",
      "Epoch 3/50\n",
      " - 22s - loss: 0.6600 - acc: 0.7550 - val_loss: 0.9999 - val_acc: 0.6486\n",
      "Epoch 4/50\n",
      " - 22s - loss: 0.6133 - acc: 0.7774 - val_loss: 1.0137 - val_acc: 0.6517\n",
      "Epoch 5/50\n",
      " - 20s - loss: 0.5865 - acc: 0.7781 - val_loss: 1.0019 - val_acc: 0.6456\n",
      "Epoch 6/50\n",
      " - 22s - loss: 0.5568 - acc: 0.7855 - val_loss: 0.9917 - val_acc: 0.6637\n",
      "Epoch 7/50\n",
      " - 22s - loss: 0.5477 - acc: 0.7915 - val_loss: 1.0212 - val_acc: 0.6757\n",
      "Epoch 8/50\n",
      " - 20s - loss: 0.5181 - acc: 0.7959 - val_loss: 0.9825 - val_acc: 0.6727\n",
      "Epoch 9/50\n",
      " - 20s - loss: 0.5090 - acc: 0.8062 - val_loss: 1.0354 - val_acc: 0.6727\n",
      "Epoch 10/50\n",
      " - 20s - loss: 0.5007 - acc: 0.8056 - val_loss: 1.0166 - val_acc: 0.6667\n",
      "Epoch 11/50\n",
      " - 20s - loss: 0.4773 - acc: 0.8139 - val_loss: 0.9909 - val_acc: 0.6577\n",
      "Epoch 12/50\n",
      " - 20s - loss: 0.4673 - acc: 0.8136 - val_loss: 1.0284 - val_acc: 0.6607\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa59f9b1e48>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=5, verbose=True)\n",
    "\n",
    "# Checkpoint - used to get the best weights during the model training process\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_weights.h5', monitor='val_acc', save_best_only=True)\n",
    "\n",
    "# Fit the pre-trained best model on the entire dataset\n",
    "best_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping, checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEmCAYAAAD1FIKpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa59be42fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best iteration\n",
    "best_model = load_model('models/best_weights.h5')\n",
    "\n",
    "# Make the predictions\n",
    "preds = best_model.predict_classes(x_test)\n",
    "preds = [i + 1 for i in preds]\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test_true, preds, labels=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[1,2,3,4,5,6,7,8,9], title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67568\n",
      "333/333 [==============================] - 1s 2ms/step\n",
      "[1.0212397825968516, 0.67567567621265445]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "acc = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print('Accuracy: %0.5f' % acc)\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in case we want to reference it later\n",
    "best_model.save('models/iteration3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
